{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis\n",
    "\n",
    "The data is split evenly with 25k reviews intended for training and 25k for testing your classifier. Moreover, each set has 12.5k positive and 12.5k negative reviews.\n",
    "\n",
    "IMDb lets users rate movies on a scale from 1 to 10. To label these reviews the curator of the data labeled anything with ≤ 4 stars as negative and anything with ≥ 7 stars as positive. Reviews with 5 or 6 stars were left out.\n",
    "\n",
    "**Import the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = []\n",
    "for line in open(os.getcwd() + '/data/imbd_train.txt', 'r', encoding='latin1'):\n",
    "    \n",
    "    reviews_train.append(line.strip())\n",
    "    \n",
    "reviews_test = []\n",
    "for line in open(os.getcwd() + '/data/imbd_test.txt', 'r', encoding='latin1'):\n",
    "    \n",
    "    reviews_test.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "####################\n",
      "Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.\n",
      "####################\n",
      "Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('####################')\n",
    "    print(reviews_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See one of the elements in the list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(reviews_train))\n",
    "print(len(reviews_test))\n",
    "reviews_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw text is pretty messy for these reviews so before we can do any analytics we need to clean things up\n",
    "\n",
    "\n",
    "**Use Regular expressions to remove the non text characters, and the html tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remover todos los signos de puntuación, exclamaciones...\n",
    "# Tb pasamos a minuscula y nos cargamos etiquetas HTML\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    # Para todas las reviews en minuscula, sustituye algunas cosas por espacio y otras por vacio.\n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# Reviews tras aplicar la limpieza\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this isn't the comedic robin williams nor is it the quirky insane robin williams of recent thriller fame this is a hybrid of the classic drama without over dramatization mixed with robin's new love of the thriller but this isn't a thriller per se this is more a mystery suspense vehicle through which williams attempts to locate a sick boy and his keeper also starring sandra oh and rory culkin this suspense drama plays pretty much like a news report until william's character gets close to achieving his goal i must say that i was highly entertained though this movie fails to teach guide inspect or amuse it felt more like i was watching a guy williams as he was actually performing the actions from a third person perspective in other words it felt real and i was able to subscribe to the premise of the story all in all it's worth a watch though it's definitely not friday saturday night fare it rates a   from the fiend \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_clean[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "In order for this data to make sense to our machine learning algorithm we’ll need to convert each review to a numeric representation, which we call vectorization.\n",
    "\n",
    "The simplest form of this is to create one very large matrix with one column for every unique word in your corpus (where the corpus is all 50k reviews in our case). Then we transform each review into one row containing 0s and 1s, where 1 means that the word in the corpus corresponding to that column appears in that review. That being said, each row of the matrix will be very sparse (mostly zeros). This process is also known as one hot encoding. Use the *CountVectorizer* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Si aparece una palabra en una review, le pone un 1. Da igual que aparezca 100 veces, no cuenta. Xq binary=True\n",
    "# Solo pone 1s cuando detecta una palabra en una review\n",
    "baseline_vectorizer = CountVectorizer(binary=True)\n",
    "baseline_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "# Reviews en formato vector de palabras. El mismo vectorizador a test, tiene que mantener la estructura\n",
    "X_baseline = baseline_vectorizer.transform(reviews_train_clean)\n",
    "X_test_baseline = baseline_vectorizer.transform(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 3410713 stored elements and shape (25000, 87063)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "\n",
    "# Asigna un numero según el orden de aparicion\n",
    "# baseline_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_c = CountVectorizer()\n",
    "vectorizer_c.fit(reviews_train_clean)\n",
    "\n",
    "# Ya no es binaria la aparicion, sino un conteo por palabra\n",
    "X_baseline_c = vectorizer_c.transform(reviews_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "87063\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline_c.shape)\n",
    "print(len(vectorizer_c.get_feature_names_out())) # Las mismas\n",
    "#X_baseline_c.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 3410713 stored elements and shape (25000, 87063)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz demasiado grande como para que numpy la imprima por pantalla\n",
    "X_baseline_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Baseline Model\n",
    "\n",
    "Train a Logistic Regression model after transforming the data with CountVectorized\n",
    "\n",
    "* They’re easy to interpret\n",
    "* Linear models tend to perform well on sparse datasets like this one\n",
    "* They learn very fast compared to other algorithms.\n",
    "\n",
    "Test models with C values of [0.01, 0.05, 0.25, 0.5, 1] and see wich is the best value for C, and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Los comentarios vienen ordenados. Los primeros 12,5k son negativos\n",
    "# A test le ocurre lo mismo\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "\n",
    "def train_model(X_TRAIN, X_TEST):\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    \n",
    "    params = {\n",
    "        'C': [0.01, 0.05, 0.25, 0.5, 1]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(lr, params, cv=5, n_jobs=-1)\n",
    "    grid.fit(X_TRAIN, target)\n",
    "\n",
    "    print (\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88184\n"
     ]
    }
   ],
   "source": [
    "train_model(X_baseline, X_test_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words\n",
    "\n",
    "Stop words are the very common words like ‘if’, ‘but’, ‘we’, ‘he’, ‘she’, and ‘they’. We can usually remove these words without changing the semantics of a text and doing so often (but not always) improves the performance of a model. Removing these stop words becomes a lot more useful when we start using longer word sequences as model features (see n-grams below).\n",
    "\n",
    "Before we apply the CountVectorized, lets remove the stopwords, included in nltk.corpus\n",
    "\n",
    "Then apply the CountVectorizer, and train the Logistic regression model and obtain the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hay que bajarse las stopwords de nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para visualizar los stopwords de inglés\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para visualizarlas en español\n",
    "stopwords.words('spanish')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Aplicamos la eliminacion de las palabras directamente sobre las reviews\n",
    "# Demasiado manual. Mejor sobre el CountVectorizer (ver abajo)\n",
    "english_stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        \n",
    "        # Para cada review elimina las stopwords, y separa todas las palabras por espacio\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() if word not in english_stop_words])\n",
    "        )\n",
    "        \n",
    "    return removed_stop_words\n",
    "\n",
    "# Se lo aplicamos antes de vectorizar\n",
    "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
    "no_stop_words_test = remove_stop_words(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vectorizamos tras eliminar las stop words\n",
    "Ver docu, tiene cosas interesantes como lowercase=True. Lo hace antes de vectorizar, \n",
    "o el argumento stopwords\n",
    "'''\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(no_stop_words_train)\n",
    "\n",
    "X = cv.transform(no_stop_words_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87046)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.879\n"
     ]
    }
   ],
   "source": [
    "# Se aplica el mismo a test\n",
    "X_test = cv.transform(no_stop_words_test)\n",
    "\n",
    "# Y entrenamos\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 87046)\n",
      "Stop words eliminadas: 17\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X_baseline tras aplicar el vectorizador tal cual en los datos\n",
    "X tras aplicar el vectorizador despues de eliminar las stop words. No se carga muchas\n",
    "'''\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87964\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "El resultado de este codigo es practicamente igual que el anterior, pero elimina más stopwords\n",
    "'''\n",
    "\n",
    "cv = CountVectorizer(binary=True,\n",
    "                     stop_words = english_stop_words)\n",
    "\n",
    "cv.fit(reviews_train_clean)\n",
    "\n",
    "X = cv.transform(reviews_train_clean)\n",
    "X_test = cv.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 86918)\n",
      "Stop words eliminadas: 145\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X_baseline tras aplicar el vectorizador tal cual en los datos\n",
    "X tras aplicar el vectorizador despus de eliminar las stop words\n",
    "En este caso elimina más, el countvectorizer tokeniza mejor las palabras\n",
    "de lo que lo hemos hecho nosotros en la funcion remove_stop_words. Por ejemplo \"it's\" serian dos palabras\n",
    "'''\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In practice, an easier way to remove stop words is to just use the stop_words argument with any of scikit-learn’s ‘Vectorizer’ classes. If you want to use NLTK’s full list of stop words you can do stop_words='english’. In practice I’ve found that using NLTK’s list actually decreases my performance because its too expansive, so I usually supply my own list of words. For example, stop_words=['in','of','at','a','the'] ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common next step in text preprocessing is to normalize the words in your corpus by trying to convert all of the different forms of a given word into one. Two methods that exist for this are Stemming and Lemmatization.\n",
    "\n",
    "# Stemming\n",
    "\n",
    "Stemming is considered to be the more crude/brute-force approach to normalization (although this doesn’t necessarily mean that it will perform worse). There’s several algorithms, but in general they all use basic rules to chop off the ends of words.\n",
    "\n",
    "NLTK has several stemming algorithm implementations. We’ll use the Porter stemmer. Most used:\n",
    "* PorterStemmer\n",
    "* SnowballStemmer\n",
    "\n",
    "Apply a PoterStemmer, vectorize, and train the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli fli flight die mule deni deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "El stemmer se aplica sobre cada palabra. Las recorta eliminando plurales y tiempos verbales\n",
    "Modifica muy poco cada palabra\n",
    "'''\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'fly','flight', 'dies', 'mules', 'denied', 'deny',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli fli flight die mule deni deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "plurals = ['caresses', 'flies', 'fly','flight', 'dies', 'mules', 'denied', 'deny',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recorr corr correl corr cas caser cas play vol vol volv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "plurals = ['recorrer', 'corriendo', 'correlación', 'correré', 'casas', 'casero', 'caso', 'playa', 'volando', 'volar', 'volveré']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8768\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos a mano. Los stemmers no eliminan palabras, solo quitan sufijos, y ahora habrá más palabras que sean iguales\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def get_stemmed_text(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "stemmed_reviews_train = get_stemmed_text(reviews_train_clean)\n",
    "stemmed_reviews_test = get_stemmed_text(reviews_test_clean)\n",
    "\n",
    "cv = CountVectorizer(binary=True, stop_words = english_stop_words)\n",
    "cv.fit(stemmed_reviews_train)\n",
    "\n",
    "X_stem = cv.transform(stemmed_reviews_train)\n",
    "X_test = cv.transform(stemmed_reviews_test)\n",
    "\n",
    "train_model(X_stem, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 66715)\n",
      "Diff X normal y X tras stemmer y vectorización: 20348\n"
     ]
    }
   ],
   "source": [
    "# No elimina palabras. Solo recorta sufijos y agrupa tipos de palabras.\n",
    "# Como resultado dará menos palabras debido al agrupado. Se carga unas cuantas letras de las palabras\n",
    "print(X_baseline.shape)\n",
    "print(X_stem.shape)\n",
    "print(\"Diff X normal y X tras stemmer y vectorización:\", X_baseline.shape[1] - X_stem.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization works by identifying the part-of-speech of a given word and then applying more complex rules to transform the word into its true root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fly fly flight dy mule study died agreed owned humbled sized meeting stating siezing itemization sensational traditional reference colonizer plotted\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "La diferencia con el stemming es que la lematización tiene en cuenta la morfología\n",
    "de la palabra, sustituyendola por la raiz, no recortándola. Y no es tan restrictivo como el stemming.\n",
    "Necesita un buen diccionario con mapeos, como wordnet\n",
    "\n",
    "En nltk no hay lematizadores en español. Habria que bajarse algun paquete como pip install es-lemmatizer\n",
    "'''\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "plurals = ['caresses', 'flies','fly','flight', 'dies', 'mules', 'studies',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [lemmatizer.lemmatize(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87824\n"
     ]
    }
   ],
   "source": [
    "def get_lemmatized_text(corpus):\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "# Lematizamos las reviews\n",
    "lemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)\n",
    "lemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)\n",
    "\n",
    "# Vectorizamos con conteo tras lematizar\n",
    "cv = CountVectorizer(binary=True, stop_words = english_stop_words)\n",
    "cv.fit(lemmatized_reviews_train)\n",
    "\n",
    "X = cv.transform(lemmatized_reviews_train)\n",
    "X_test = cv.transform(lemmatized_reviews_test)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 80215)\n",
      "Diff X normal y X tras lematizador y vectorización: 6848\n"
     ]
    }
   ],
   "source": [
    "# Elimina menos que con el stemmer. Normal, el stemmer recorta mucho del sufijo\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams\n",
    "\n",
    "We can potentially add more predictive power to our model by adding two or three word sequences (bigrams or trigrams) as well. For example, if a review had the three word sequence “didn’t love movie” we would only consider these words individually with a unigram-only model and probably not capture that this is actually a negative sentiment because the word ‘love’ by itself is going to be highly correlated with a positive review.\n",
    "\n",
    "The scikit-learn library makes this really easy to play around with. Just use the ngram_range argument with any of the ‘Vectorizer’ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"didn't\", 'love')\n",
      "('love', 'music')\n",
      "('music', 'at')\n",
      "('at', 'all')\n",
      "('all', 'my')\n",
      "('my', 'love')\n",
      "###############\n",
      "(\"didn't\", 'love', 'music')\n",
      "('love', 'music', 'at')\n",
      "('music', 'at', 'all')\n",
      "('at', 'all', 'my')\n",
      "('all', 'my', 'love')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"didn't love music at all my love\"\n",
    "\n",
    "two = ngrams(sentence.split(), 2)\n",
    "three = ngrams(sentence.split(), 3)\n",
    "\n",
    "for grams in two:\n",
    "  print(grams)\n",
    "print('###############')\n",
    "for grams in three:\n",
    "  print(grams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Puede ser bigramas si ngram_range=(2,2), o trigramas (3,3)...\n",
    "Algunas palabras las elimina, como 'a'. Cuidado con eso a la hora de hacer el conteo\n",
    "ngram_range=(1, 3) significa las palabras por separado, los bigramas y los trigramas\n",
    "Ojo que esto aumenta muchisimo el espacio de features\n",
    "'''\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                    ngram_range=(1, 2))\n",
    "\n",
    "vector = ngram_vectorizer.fit_transform([sentence]).toarray()\n",
    "print(vector)\n",
    "print(len(vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88956\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Va como argumento del CountVectorizer\n",
    "'''\n",
    "ngram_vectorizer = CountVectorizer(binary=True, stop_words = english_stop_words,\n",
    "                                    ngram_range=(1, 2))\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 1865232)\n",
      "Diff X normal y X tras lematizador y vectorización: -1778169\n"
     ]
    }
   ],
   "source": [
    "# Añade 1448047 n gramas. Cuanto mas ngramas, mayor será el espacio de features\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 1778314)\n",
      "Diff X normal y X tras lematizador y vectorización: -1691251\n",
      "Final Accuracy: 0.84028\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Va como argumento del CountVectorizer\n",
    "'''\n",
    "ngram_vectorizer = CountVectorizer(binary=True, stop_words = english_stop_words,\n",
    "                                    ngram_range=(2, 2))\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Another common way to represent each document in a corpus is to use the tf-idf statistic (term frequency-inverse document frequency) for each word, which is a weighting factor that we can use in place of binary or word count representations.\n",
    "\n",
    "There are several ways to do tf-idf transformation but in a nutshell, **tf-idf aims to represent the number of times a given word appears in a document (a movie review in our case) relative to the number of documents in the corpus that the word appears in**.\n",
    "\n",
    "**Note:** Now that we’ve gone over n-grams, when I refer to ‘words’ I really mean any n-gram (sequence of words) if the model is using an n greater than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.2876820724517808)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ln(N + 1 / count + 1) + 1\n",
    "Cuanto mas comun, menor es el TDFIDF. Cuanto mas rara, mayor\n",
    "'''\n",
    "# Numero de documentos\n",
    "N = 3\n",
    "\n",
    "# Numero de veces que aparece\n",
    "count = 2\n",
    "\n",
    "1 + np.log((N + 1)/(count + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.69314718 1.28768207 1.69314718 1.69314718 1.        ]\n",
      "['fat' 'is' 'my' 'name' 'ralph']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TfidfTransformer\n",
    "'''\n",
    "Cuanto mas comun, mas bajo es el TfidfVectorizer\n",
    "'''\n",
    "sent1 = 'My name is Ralph'\n",
    "sent2 = 'Ralph is fat'\n",
    "sent3 = 'Ralph'\n",
    "\n",
    "test = TfidfVectorizer()\n",
    "test.fit_transform([sent1, sent2, sent3])\n",
    "print(test.idf_)\n",
    "print(test.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.6931471805599454)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + np.log((3 + 1)/(1 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(reviews_train_clean)\n",
    "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
    "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 87063)\n",
      "Diff X normal y X tras lematizador y vectorización: 0\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "Recall that linear classifiers tend to work well on very sparse datasets (like the one we have). Another algorithm that can produce great results with a quick training time are Support Vector Machines with a linear kernel.\n",
    "\n",
    "Build a model with an n-gram range from 1 to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.89768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SVM con bigramas\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "\n",
    "\n",
    "def train_model_svm(X_TRAIN, X_TEST):\n",
    "    \n",
    "    svm = LinearSVC()\n",
    "    \n",
    "    params = {\n",
    "        'C': [0.01, 0.05, 0.25, 0.5, 1]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(svm, params, cv=5, n_jobs=-1)\n",
    "    grid.fit(X_TRAIN, target)\n",
    "\n",
    "    print (\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))\n",
    "    \n",
    "\n",
    "train_model_svm(X, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "Removing a small set of stop words along with an n-gram range from 1 to 3 and a linear support vector classifier shows the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                    ngram_range=(1, 2),\n",
    "                                    stop_words=stop_words)\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Postitive and Negative Features\n",
    "\n",
    "Obtain the most important features of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87063\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(reviews_train_clean)\n",
    "X = cv.transform(reviews_train_clean)\n",
    "\n",
    "log_reg = LogisticRegression(C=0.5)\n",
    "log_reg.fit(X, target)\n",
    "\n",
    "# Importancia de los coeficientes. En total, todas las palabras vectorizadas\n",
    "print(len(log_reg.coef_[0]))\n",
    "\n",
    "# Cada coeficiente va asociado a una palabra\n",
    "cv.get_feature_names_out()\n",
    "\n",
    "# Montamos un diccionario con palabra -> coeficiente\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names_out(), log_reg.coef_[0]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('excellent', np.float64(1.3824276911090625))\n",
      "('refreshing', np.float64(1.2817893118767159))\n",
      "('perfect', np.float64(1.205742921120973))\n",
      "('superb', np.float64(1.1536195719726952))\n",
      "('appreciated', np.float64(1.133917467837408))\n",
      "################################\n",
      "('worst', np.float64(-2.075626262928442))\n",
      "('waste', np.float64(-1.9199689605859493))\n",
      "('disappointment', np.float64(-1.6946209822433387))\n",
      "('poorly', np.float64(-1.6776717517060358))\n",
      "('awful', np.float64(-1.5408757475689352))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:5]:\n",
    "    print(best_positive)\n",
    "    \n",
    "print('################################')\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:5]:\n",
    "    print(best_negative)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
