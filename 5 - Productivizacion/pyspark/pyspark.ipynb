{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e8885a",
   "metadata": {},
   "source": [
    "# PySpark (Apache Spark con Python)\n",
    "\n",
    "## 0. Introducción y motivación – ¿Por qué usar PySpark/Spark?\n",
    "\n",
    "Apache Spark es un motor de **análisis de datos a gran escala** que permite el procesamiento distribuido de grandes conjuntos de datos de forma rápida y sencilla. Fue diseñado para superar las limitaciones de tecnologías previas como Hadoop MapReduce, ofreciendo un procesamiento **en memoria** mucho más veloz y una interfaz de alto nivel en múltiples lenguajes (Scala, Java, Python, R). Su API para Python es conocida como **PySpark**, y nos permite aprovechar la potencia de Spark usando el lenguaje Python. A diferencia de bibliotecas como Pandas (que operan en un solo equipo y en memoria), PySpark está pensado para **escalar horizontalmente** en clústeres de múltiples nodos, siendo ideal para trabajar con volúmenes de datos que no caben en la memoria de un solo ordenador o para acelerar cómputos mediante paralelismo masivo.\n",
    "\n",
    "**¿Cuándo conviene usar PySpark en lugar de Pandas o SQL?** Cuando los datos son muy grandes o las operaciones muy costosas. Pandas funciona en memoria local y es excelente para datasets que caben en RAM y para análisis rápidos en un solo equipo, pero cuando los datos crecen (millones de registros o más) Pandas se vuelve lento o imposible de usar por límites de memoria. Spark, en cambio, distribuye los datos y el cómputo en varios nodos, pudiendo manejar desde gigabytes hasta petabytes de información. Internamente, Spark optimiza las consultas mediante planificación de tareas (DAG scheduler) y evalúa las operaciones **de forma diferida (lazy evaluation)**: es decir, construye un plan de ejecución y solo procesa los datos cuando se le solicita un resultado (una *acción*). Esto permite optimizar todo el flujo antes de ejecutar, aplicando técnicas como *pipelining* y combinando operaciones, lo que resulta en un rendimiento muy alto en cargas de trabajo intensivas.\n",
    "\n",
    "Además, Spark es un entorno unificado que incluye varios módulos integrados (SQL/DataFrames, streaming en tiempo real, machine learning con MLlib, gráficos con GraphX, etc.), por lo que es muy versátil. En contexto empresarial, Spark se ha convertido en un estándar de facto para **Big Data**, permitiendo ejecutar análisis complejos en clusters (ya sea on-premise o en la nube) con relativa facilidad. PySpark en particular es popular entre científicos de datos porque permite usar Python (y sus librerías como Pandas, NumPy, etc.) en entornos distribuidos, integrándose bien con herramientas existentes.\n",
    "\n",
    "En resumen, **las ventajas clave de Spark/PySpark** son:\n",
    "\n",
    "* **Velocidad**: Usa procesamiento en memoria y ejecución distribuida, logrando órdenes de magnitud más rapidez que enfoques basados en disco (como Hadoop MapReduce) especialmente en algoritmos iterativos y consultas interactivas. Su motor optimiza las consultas antes de ejecutarlas y aprovecha todos los núcleos disponibles en el clúster.\n",
    "* **Escalabilidad horizontal**: Puede manejar grandes volúmenes de datos repartiendo el trabajo entre múltiples nodos del cluster. Basta con añadir más nodos para aumentar la capacidad de cómputo y almacenamiento, logrando escalar a *big data* real. El mismo código PySpark que corre en tu portátil con una pequeña muestra puede ejecutarse en un clúster de decenas o cientos de máquinas sobre toda la data.\n",
    "* **Facilidad de uso y familiaridad**: Ofrece APIs de alto nivel similares a las de Pandas o SQL, lo que hace más sencillo para desarrolladores y analistas adoptar Spark. Puedes escribir consultas en PySpark de forma declarativa (por ejemplo, usando operaciones estilo DataFrame o incluso sentencias SQL) sin tener que preocuparte por detalles bajos de paralelismo. Además, al ser Python, es posible reutilizar conocimiento de bibliotecas como Pandas (incluso existe una API Pandas sobre Spark, que veremos más adelante).\n",
    "* **Confiabilidad y tolerancia a fallos**: Spark maneja automáticamente la tolerancia a fallos mediante la replicación de datos (con su abstracción de RDDs - Resilient Distributed Datasets) y recomputación de particiones en caso de que algún nodo falle. El desarrollador no tiene que gestionar manualmente estos detalles.\n",
    "* **Ecosistema y versatilidad**: Spark no solo sirve para SQL batch, sino que soporta procesamiento de streams en tiempo real, machine learning distribuido, y más, todo en el mismo framework. Esto evita tener que integrar múltiples herramientas distintas para casos de uso diferentes.\n",
    "\n",
    "En definitiva, PySpark nos permite **lo mejor de dos mundos**: escribir código en Python de alto nivel (similar a Pandas/SQL) pero con capacidad de ejecutarse de forma distribuida sobre grandes volúmenes de datos. Si tu dataset es pequeño (digamos, menos de unos pocos millones de filas), Pandas probablemente sea más sencillo y suficiente. Pero si estás manejando datos **“grandes” (big data)** o quieres aprovechar un cluster de cómputo para acelerar tareas, PySpark es una excelente elección.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e4e02",
   "metadata": {},
   "source": [
    "\n",
    "&#x20;*Arquitectura de un clúster Spark:* el programa *driver* (que contiene el `SparkContext` dentro de nuestra aplicación PySpark) se comunica con un *Cluster Manager* para solicitar recursos, y lanza procesos *executor* en los nodos trabajadores del clúster. Los *executors* ejecutan las tareas en paralelo y almacenan datos en memoria o disco (cache) según sea necesario. Gracias a esta arquitectura maestro-trabajador, Spark distribuye la carga de trabajo entre los nodos, manteniendo los datos redundantes para tolerancia a fallos y escalando eficientemente.\n"
   ]
  },
  {
   "attachments": {
    "Sin título.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "f016939a",
   "metadata": {},
   "source": [
    "![Sin título.png](<attachment:Sin título.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c2d81",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Instalación inicial: entorno con Docker y PySpark\n",
    "\n",
    "Para empezar a trabajar con PySpark, configuraremos un entorno local que simula un clúster Spark usando **Docker**, mientras ejecutamos Python (nuestro driver) desde fuera del contenedor. De esta forma, podemos tener fácilmente un *cluster* Spark en nuestra máquina de desarrollo sin instalar Spark directamente en el host. Los pasos generales serán:\n",
    "\n",
    "* **Instalar Java en el sistema host** (donde correrá PySpark): Spark corre sobre la JVM, por lo que incluso usando PySpark es necesario tener Java instalado en la máquina donde ejecutamos el driver. Si no se tiene Java, PySpark no iniciará (lanzará errores de `JAVA_HOME` no definido). Asegúrate de tener Java 8+ instalado y la variable de entorno `JAVA_HOME` configurada apuntando al JRE/JDK instalado.\n",
    "\n",
    "* **Descargar una imagen Docker de Apache Spark**: Usaremos una imagen preconfigurada que incluya Spark. Una opción recomendada es la imagen oficial de Bitnami, que ya viene con Spark y Python instalados. Por ejemplo, podemos utilizar la versión 3.3.1 de Spark con Python 3.8 ejecutando:\n",
    "\n",
    "  ```bash\n",
    "  docker pull bitnami/spark:3.3.1\n",
    "  ```\n",
    "\n",
    "  > *Nota:* Es importante elegir una imagen cuya versión de Spark y Python coincida con la que usaremos en nuestro código PySpark. Por ejemplo, la imagen `bitnami/spark:3.3.1` trae Spark 3.3.1 y Python 3.8; por tanto, en nuestro entorno Python deberemos instalar `pyspark==3.3.1` y usar Python 3.8 para evitar incompatibilidades de serialización entre el driver y los executors. Si usamos versiones que no coinciden exactamente (por ejemplo Spark 3.3 en el clúster pero pyspark 3.2 en el driver), es probable que veamos errores de incompatibilidad al enviar tareas al cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc1c5c",
   "metadata": {},
   "source": [
    "\n",
    "* **Lanzar un clúster Spark en contenedores**: Vamos a ejecutar dos contenedores: uno actuará como **Master** de Spark y otro como **Worker** (ejecutor). Esto constituye un mini-clúster Spark *standalone*. Primero, crea una red Docker para que los contenedores se comuniquen:\n",
    "\n",
    "  ```bash\n",
    "  docker network create spark-net\n",
    "  ```\n",
    "\n",
    "  Luego ejecuta el contenedor del *master*:\n",
    "\n",
    "  ```bash\n",
    "  docker run -d --name spark-master \\\n",
    "    -p 8080:8080 -p 7077:7077 \\\n",
    "    --network spark-net \\\n",
    "    -e SPARK_MODE=master \\\n",
    "    bitnami/spark:3.3.1\n",
    "  ```\n",
    "\n",
    "  Este comando inicia el Master de Spark en modo standalone. Mapeamos el puerto 8080 del contenedor al 8080 local (para acceder al UI web del master) y el puerto 7077 (puerto donde el master escucha las solicitudes de conexión de drivers y workers).  A continuación, iniciamos uno o más *workers* que se conecten al master:\n",
    "\n",
    "  ```bash\n",
    "  docker run -d --name spark-worker-1 \\\n",
    "    --network spark-net \\\n",
    "    -e SPARK_MODE=worker \\\n",
    "    -e SPARK_MASTER_URL=spark://spark-master:7077 \\\n",
    "    bitnami/spark:3.3.1\n",
    "  ```\n",
    "\n",
    "  Este contenedor iniciará un proceso Worker que se registrará en el Master (cuyo hostname en la red de Docker es `spark-master`). Puedes repetir este comando (cambiando el nombre del contenedor) para lanzar múltiples workers si lo deseas. Con esto, ya tenemos un clúster Spark en marcha: un master y (al menos) un worker ejecutándose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13260f09",
   "metadata": {},
   "source": [
    "\n",
    "* **Preparar el entorno Python (driver)**: En tu máquina host (o donde ejecutes el notebook de Jupyter), instala la misma versión de PySpark y Python que la imagen Docker. Si usas conda o venv, crea un entorno con, por ejemplo, Python 3.8 y PySpark 3.3.1:\n",
    "\n",
    "  ```bash\n",
    "  pip install pyspark==3.3.1\n",
    "  ```\n",
    "\n",
    "  (*Si estás en un Jupyter Notebook, también puedes instalarlo directamente en la celda.*) Esto nos permitirá usar `pyspark` en nuestro código para conectarnos al cluster.\n",
    "\n",
    "* **Conectar PySpark al cluster**: Ya con el cluster corriendo y PySpark instalado en el host, podemos escribir un script o notebook Python y crear una SparkSession apuntando al master. Por ejemplo, en Python:\n",
    "\n",
    "  ```python\n",
    "  from pyspark.sql import SparkSession\n",
    "\n",
    "  spark = SparkSession.builder \\\n",
    "      .appName(\"PySparkTutorial\") \\\n",
    "      .master(\"spark://localhost:7077\") \\\n",
    "      .getOrCreate()\n",
    "  ```\n",
    "\n",
    "  Aquí usamos la dirección `spark://localhost:7077` porque expusimos el puerto 7077 de nuestro master al host local. Esto inicializa una **SparkSession**, el punto de entrada principal para trabajar con Spark SQL y DataFrames. La SparkSession representa la conexión a nuestro cluster Spark; al crearla, Spark buscará al master en esa dirección y registrará nuestra aplicación (**appName**) en el cluster. Si todo está correcto, la sesión se creará y el master asignará recursos (por defecto, usará todos los núcleos disponibles en los workers, pero esto se puede configurar).\n",
    "\n",
    "  > *Tip:* Si en lugar de un cluster quisiéramos correr Spark en modo local (sin Docker), podríamos usar `.master(\"local[*]\")` para indicar que use todos los núcleos locales. Esto lanza procesos *executor* en hilos del mismo proceso Python. Este modo es útil para pruebas rápidas, pero carece de los beneficios de escalado del cluster.\n",
    "\n",
    "* **Verificar la instalación**: Podemos probar una operación sencilla para comprobar que todo funciona. Por ejemplo, contar números de 1 a 1000 en paralelo:\n",
    "\n",
    "  ```python\n",
    "  rdd = spark.sparkContext.parallelize(range(1, 1001))\n",
    "  print(\"Cuenta:\", rdd.count())\n",
    "  ```\n",
    "\n",
    "  Si la instalación es correcta, esta operación distribuirá la secuencia en el cluster, contará los elementos y devolverá `1000` como resultado (el número de elementos). Al ejecutarla, puedes visitar en tu navegador `http://localhost:8080` para ver la interfaz web del Spark Master, donde debería aparecer tu aplicación activa, y también `http://localhost:4040` para ver la interfaz web del driver (Spark UI) con detalles de la tarea ejecutada.\n",
    "\n",
    "En este punto, tenemos nuestro entorno PySpark listo: un cluster Spark corriendo en Docker y una sesión Spark accesible desde Python. A continuación, empezaremos a explorar las funcionalidades de PySpark a través de su API de **DataFrames**, comparándola con herramientas conocidas como Pandas y SQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff6937",
   "metadata": {},
   "source": [
    "\n",
    "## 2. DataFrames y estructuras de datos en PySpark\n",
    "\n",
    "En Spark, la principal abstracción de alto nivel para manejar datos estructurados es el **DataFrame**. Un DataFrame de Spark es muy similar a una tabla en una base de datos relacional o a un DataFrame de Pandas: es decir, es una colección de datos organizados en filas y columnas, con un esquema bien definido (nombres de columnas y tipos de datos). Internamente, los DataFrames de PySpark están respaldados por RDDs (Resilient Distributed Datasets), pero expuestos a usuario con una API mucho más cómoda y optimizada, incluyendo muchas operaciones de selección, filtrado, agregación, etc., todo de forma *declarativa*.\n",
    "\n",
    "¿Por qué usar DataFrames y no RDDs directamente? Porque Spark puede aplicar optimizaciones avanzadas (a través de un componente llamado **Catalyst Optimizer**) cuando trabajamos con DataFrames y consultas SQL, generando planes de ejecución eficientes. Además, el código es más conciso y fácil de leer. De hecho, en la mayoría de los casos de uso (ETL, análisis de datos, machine learning), se recomienda usar DataFrames/Datasets en lugar de RDDs puros, ya que **son más sencillos y suelen rendir mejor**. Solo en situaciones muy específicas de bajo nivel conviene manejar RDDs directamente.\n",
    "\n",
    "**Creación de una SparkSession**: Antes de crear o cargar cualquier DataFrame, necesitamos tener una `SparkSession` activa (como hicimos en la sección anterior). En un entorno interactivo (por ejemplo, usando el shell `pyspark` o algunas distribuciones), la SparkSession a veces ya existe pre-creada. En nuestros scripts o notebooks, como vimos, la creamos manualmente. Recordemos importar la clase necesaria:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"spark://localhost:7077\").appName(\"TutorialPySpark\").getOrCreate()\n",
    "```\n",
    "\n",
    "*(Ajusta el master y appName según tu configuración; si sigues con el ejemplo Docker anterior, `spark://localhost:7077` está bien. Si estás en modo local puro sin cluster, usarías `\"local[*]\"`.)*\n",
    "\n",
    "Una vez tenemos la SparkSession (`spark`), ya podemos crear DataFrames.\n",
    "\n",
    "### Cargando datos en un DataFrame\n",
    "\n",
    "Existen múltiples formas de obtener un DataFrame en PySpark:\n",
    "\n",
    "* **Leer desde un archivo o fuente de datos** (CSV, JSON, Parquet, una tabla SQL, etc.).\n",
    "* **Convertir una colección de objetos locales o Pandas DataFrame** a un DataFrame distribuido de Spark.\n",
    "* **Partir de un RDD existente** y transformarlo a DataFrame indicando un esquema.\n",
    "\n",
    "Veamos ejemplos comunes:\n",
    "\n",
    "**a) Leer un archivo CSV/JSON**: La SparkSession proporciona lectores para distintos formatos. Por ejemplo, si tenemos un archivo CSV con datos, podemos hacer:\n",
    "\n",
    "```python\n",
    "# Leer un CSV con inferencia de esquema y cabeceras\n",
    "df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"ruta/al/archivo.csv\")\n",
    "```\n",
    "\n",
    "Esto carga el CSV en un DataFrame `df`, intentando inferir automáticamente el tipo de dato de cada columna y tratando la primera línea como cabecera (nombres de columna). Alternativamente, podríamos especificar manualmente un esquema o ajustar opciones (separador, formato de fecha, codificación, etc.) según el caso. PySpark también permite leer JSON (`spark.read.json(\"file.json\")`), Parquet (`spark.read.parquet(...)`), avro, etc., así como fuentes externas (por ejemplo, una tabla en Hive, o mediante JDBC a una base SQL).\n",
    "\n",
    "**b) A partir de datos en Python (lista, diccionario, Pandas)**: Si ya tenemos datos en una estructura Python, podemos crear un DataFrame directamente. Un caso útil es cuando tienes un DataFrame de **Pandas** y quieres pasarlo a Spark (por ejemplo, porque inicialmente los datos cabían en Pandas pero ahora quieres distribuirlos):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Ejemplo: dataset \"iris\" de scikit-learn cargado en pandas\n",
    "iris_pd = pd.DataFrame({\n",
    "    \"sepal_length\": [5.1, 4.9, 4.7, 4.6, 5.0],\n",
    "    \"sepal_width\":  [3.5, 3.0, 3.2, 3.1, 3.6],\n",
    "    \"petal_length\": [1.4, 1.4, 1.3, 1.5, 1.4],\n",
    "    \"petal_width\":  [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "    \"species\":      [\"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"]\n",
    "})\n",
    "print(iris_pd.head())\n",
    "#   sepal_length  sepal_width  petal_length  petal_width species\n",
    "# 0           5.1          3.5           1.4          0.2  setosa\n",
    "# 1           4.9          3.0           1.4          0.2  setosa\n",
    "# ...\n",
    "```\n",
    "\n",
    "Tenemos un pequeño DataFrame de Pandas con cinco filas de ejemplo (columnas numéricas y una categórica de especie). Para convertirlo a un DataFrame Spark:\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame(iris_pd)\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "Cuando usamos `createDataFrame` con un pandas.DataFrame, PySpark utiliza Arrow (si está disponible) para convertirlo de manera eficiente. El resultado `df` es un DataFrame distribuido (aunque con estos pocos datos seguirá siendo solo una partición). El `printSchema()` nos mostraría el esquema inferido:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- sepal_length: double (nullable = true)\n",
    " |-- sepal_width: double (nullable = true)\n",
    " |-- petal_length: double (nullable = true)\n",
    " |-- petal_width: double (nullable = true)\n",
    " |-- species: string (nullable = true)\n",
    "```\n",
    "\n",
    "Como vemos, Spark infiere los tipos: las columnas numéricas son *double* (coma flotante de 64 bits) y la columna de texto es *string*. Todas aparecen como *nullable = true* (Spark permite por defecto nulos en todas las columnas a menos que se especifique lo contrario).\n",
    "\n",
    "También podríamos crear un DataFrame a partir de, por ejemplo, una lista de tuplas o un RDD. Por ejemplo:\n",
    "\n",
    "```python\n",
    "data = [(\"Ana\", 34), (\"Juan\", 45), (\"Pedro\", 23)]\n",
    "df2 = spark.createDataFrame(data, schema=[\"nombre\", \"edad\"])\n",
    "df2.show()\n",
    "# +------+---+\n",
    "# |nombre|edad|\n",
    "# +------+---+\n",
    "# |   Ana| 34 |\n",
    "# |  Juan| 45 |\n",
    "# |  Pedro| 23 |\n",
    "# +------+---+\n",
    "```\n",
    "\n",
    "Aquí pasamos una lista de tuplas y definimos los nombres de columna en el esquema. Spark asignó tipos (edad como long, nombre como string). Si no pasamos schema, Spark habría dado nombres genéricos `_1`, `_2`, ... a las columnas.\n",
    "\n",
    "**c) Esquemas explícitos**: En casos avanzados, podemos crear esquemas usando tipos de datos de Spark (`pyspark.sql.types.StructType`, `StructField`, etc.) para definir exactamente los tipos y nombres, y luego aplicarlos al crear el DataFrame. Esto es útil cuando queremos asegurarnos de los tipos o para CSVs sin cabecera, etc. En la mayoría de situaciones, inferir o usar schema simples como listas de nombres es suficiente.\n",
    "\n",
    "Una vez obtenido un DataFrame, es importante entender que **las transformaciones en Spark son *vagas (lazy)* y los DataFrames son inmutables**. Al aplicar operaciones (filtros, selects, etc.) no se ejecutan inmediatamente, sino que Spark construye internamente un plan (un grafo de operaciones) y postergará la ejecución hasta que necesitemos un resultado concreto (por ejemplo al hacer un `show()` o un `count()` que requiera calcular algo). Asimismo, cada transformación genera un *nuevo* DataFrame (no modifica el existente), dado que los DataFrames (y RDDs) son inmutables. Esto difiere de Pandas, donde sí podemos modificar un DataFrame en memoria. En PySpark, si haces `df2 = df.filter(...)`, `df` sigue existiendo igual; `df2` es un nuevo DataFrame con la transformación aplicada. Esta naturaleza inmutable facilita la tolerancia a fallos y la reproducción de cálculos en el cluster.\n",
    "\n",
    "### Exploración inicial del DataFrame\n",
    "\n",
    "Al igual que en Pandas usamos `df.head()` o `df.info()`, en PySpark tenemos algunas acciones para inspeccionar el contenido y estructura:\n",
    "\n",
    "* `df.show(n)` – Muestra las primeras *n* filas del DataFrame (20 por defecto) en formato tabular, útil para vistazo rápido. Por ejemplo `df.show(5)`.\n",
    "* `df.printSchema()` – Muestra el esquema (estructuras de columnas y tipos).\n",
    "* `df.columns` – Lista con los nombres de las columnas.\n",
    "* `df.describe().show()` – Calcula estadísticas descriptivas básicas (count, mean, stddev, min, max) para columnas numéricas, similar a `pandas.DataFrame.describe`.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "```python\n",
    "df.show(5)\n",
    "# +------------+-----------+------------+-----------+-------+\n",
    "# |sepal_length|sepal_width|petal_length|petal_width|species|\n",
    "# +------------+-----------+------------+-----------+-------+\n",
    "# |         5.1|        3.5|         1.4|        0.2| setosa|\n",
    "# |         4.9|        3.0|         1.4|        0.2| setosa|\n",
    "# |         4.7|        3.2|         1.3|        0.2| setosa|\n",
    "# |         4.6|        3.1|         1.5|        0.2| setosa|\n",
    "# |         5.0|        3.6|         1.4|        0.2| setosa|\n",
    "# +------------+-----------+------------+-----------+-------+\n",
    "```\n",
    "\n",
    "Tenemos 5 filas del DataFrame iris con las columnas que definimos. Observa que el método `show()` es el que **dispara realmente la ejecución**: Spark leerá o calculará los datos necesarios para mostrar esas filas. Si el dataset fuera enorme, `show(5)` solo trae 5 filas (lo justo para mostrarlas), lo cual es práctico para examinar datos sin volcar todo el dataset.\n",
    "\n",
    "Con el DataFrame listo, pasemos a realizar operaciones comunes: **seleccionar columnas, filtrar filas, crear columnas derivadas, agrupar, ordenar, juntar dataframes, etc.** Iremos comparando con cómo se haría en Pandas o SQL cuando sea relevante.\n",
    "\n",
    "## 3. Operaciones de *slicing* y filtrado de datos\n",
    "\n",
    "Una de las tareas más básicas en análisis de datos es **seleccionar un subconjunto de columnas o filas** que nos interesan. En Pandas haríamos cosas como `df[[\"col1\",\"col2\"]]` para columnas o `df[df[\"col\"] > 5]` para filtrar filas. Veamos cómo lograr equivalentes en PySpark.\n",
    "\n",
    "### Seleccionar columnas (*slicing* de columnas)\n",
    "\n",
    "Para elegir columnas específicas de un DataFrame de Spark, usamos principalmente el método `.select()`. Ejemplos:\n",
    "\n",
    "```python\n",
    "# Seleccionar solo un subconjunto de columnas\n",
    "df_subset = df.select(\"sepal_length\", \"sepal_width\", \"species\")\n",
    "df_subset.show(5)\n",
    "# +------------+-----------+-------+\n",
    "# |sepal_length|sepal_width|species|\n",
    "# +------------+-----------+-------+\n",
    "# |         5.1|        3.5| setosa|\n",
    "# |         4.9|        3.0| setosa|\n",
    "# |         4.7|        3.2| setosa|\n",
    "# |         4.6|        3.1| setosa|\n",
    "# |         5.0|        3.6| setosa|\n",
    "# +------------+-----------+-------+\n",
    "```\n",
    "\n",
    "También podemos **renombrar columnas** en la selección usando `alias` (o posteriormente con `withColumnRenamed`, que veremos). Por ejemplo, digamos que queremos renombrar `sepal_length` a `sepal_len` solo en la vista seleccionada:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"sepal_length\").alias(\"sepal_len\"), col(\"species\")).show(3)\n",
    "# +---------+-------+\n",
    "# |sepal_len|species|\n",
    "# +---------+-------+\n",
    "# |      5.1| setosa|\n",
    "# |      4.9| setosa|\n",
    "# |      4.7| setosa|\n",
    "# +---------+-------+\n",
    "```\n",
    "\n",
    "*(Aquí importamos `col` de `pyspark.sql.functions` para referirnos a columnas por nombre; también podríamos usar directamente strings en select como antes.)*\n",
    "\n",
    "Spark no soporta exactamente la sintaxis `df[\"col\"]` para obtener un subconjunto de DataFrame como hace Pandas. En Spark, `df[\"col\"]` devuelve un objeto `Column` (una representación de la columna para usar en expresiones). De hecho, una forma alternativa de seleccionar es pasar esa columna a `.select` o usar `df.select(df[\"col1\"], df[\"col2\"])`. Pero la forma más clara suele ser `df.select(\"col1\",\"col2\",...)` o usando `col(\"col\")` para funciones más complejas.\n",
    "\n",
    "Si queremos seleccionar **todas las columnas excepto algunas**, podemos usar `df.drop(\"colA\",\"colB\")` para quitar columnas no deseadas. Por ejemplo `df.drop(\"target\").printSchema()` quitaría la columna target del esquema. También existe un atajo `df.select(\"*\")` para indicar \"todas las columnas\".\n",
    "\n",
    "### Filtrado de filas por condición\n",
    "\n",
    "El filtrado en PySpark se realiza usando el método `.filter()` o su alias `.where()`, pasando una expresión booleana. A diferencia de Pandas, **no** podemos usar la notación `df[df.col > 5]` directamente; tenemos que construir la condición con las Column de PySpark. Ejemplos:\n",
    "\n",
    "Supongamos que en nuestro DataFrame `df` de iris queremos las filas donde `sepal_length > 5.0`. En PySpark:\n",
    "\n",
    "```python\n",
    "df_filtered = df.filter(df.sepal_length > 5.0)\n",
    "df_filtered.show(5)\n",
    "# (muestra 5 filas donde sepal_length es mayor a 5.0)\n",
    "```\n",
    "\n",
    "También podríamos escribir la condición con `col`:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"sepal_length\") > 5.0).show(5)\n",
    "```\n",
    "\n",
    "Ambos son equivalentes. Incluso es posible pasar la condición como string estilo SQL, por ejemplo `df.filter(\"sepal_length > 5.0 AND species = 'virginica'\")`, aunque perderíamos el chequeo de errores en tiempo de compilación (es menos seguro si el string está mal escrito). Usar `df.colname` o `col(\"name\")` es más común.\n",
    "\n",
    "**Condiciones compuestas:** Para combinar varias condiciones, utilizamos los operadores `&` (AND lógico) y `|` (OR lógico), recordando envolver cada comparación entre paréntesis. Por ejemplo, filas donde `sepal_length > 5.0` **y** la `species` sea *virginica*:\n",
    "\n",
    "```python\n",
    "df.filter( (df.sepal_length > 5.0) & (df.species == \"virginica\") ).show(5)\n",
    "```\n",
    "\n",
    "O filas donde `species` sea *setosa* **o** *versicolor*:\n",
    "\n",
    "```python\n",
    "df.filter( (df.species == \"setosa\") | (df.species == \"versicolor\") ).show()\n",
    "```\n",
    "\n",
    "Recuerda que en PySpark usamos `==` para comparar columnas a valores (no un solo `=`), y que los operadores son sobrecargados para producir Columnas booleanas. **¡Atención!:** en Pandas uno a veces olvida poner los paréntesis o usa `and`/`or` en vez de `&`/`|`, lo cual en Pandas ya causa error. En PySpark es obligatorio usar `&` y `|` bit a bit, y siempre con cada comparación entre `(...)`.\n",
    "\n",
    "También existe la función `df.where(...)` que es exactamente lo mismo que `filter` (alias semántico, para quien prefiera la analogía SQL de *WHERE*).\n",
    "\n",
    "Veamos un ejemplo concreto con nuestro dataset iris: queremos filtrar las muestras de *versicolor* con anchura de sépalo mayor a 3:\n",
    "\n",
    "```python\n",
    "df.filter( (df.species == \"versicolor\") & (df.sepal_width > 3.0) ).show()\n",
    "# +------------+-----------+------------+-----------+----------+\n",
    "# |sepal_length|sepal_width|petal_length|petal_width|   species|\n",
    "# +------------+-----------+------------+-----------+----------+\n",
    "# |         7.0|        3.2|         4.7|        1.4|versicolor|\n",
    "# |         6.4|        3.2|         4.5|        1.5|versicolor|\n",
    "# |         6.9|        3.1|         4.9|        1.5|versicolor|\n",
    "# ... (etc)\n",
    "```\n",
    "\n",
    "**Filtrar valores nulos:** Si queremos eliminar filas con valores nulos en ciertas columnas, PySpark ofrece métodos específicos (`df.dropna()`, con opciones para umbrales, subset de columnas, etc.). Por ejemplo, `df.dropna(subset=[\"col1\",\"col2\"])` eliminaría filas donde *col1* o *col2* sean nulos. Para reemplazar nulos por un valor dado, está `df.fillna(valor, subset=[...])`. En nuestro dataset de ejemplo no tenemos nulos, pero es muy común en la práctica.\n",
    "\n",
    "**Eliminar duplicados:** Para quitar duplicados podemos usar `df.distinct()` que devuelve las filas únicas considerando todas las columnas, o `df.dropDuplicates([\"colA\",\"colB\"])` para considerar solo un subset como clave. Esto es análogo a un `SELECT DISTINCT` en SQL o `df.drop_duplicates()` en Pandas.\n",
    "\n",
    "Ejemplo rápido: si tuviésemos duplicados en iris (no debería haber, pero supongamos), `df.select(\"species\").distinct().show()` nos daría las especies únicas (setosa, versicolor, virginica). Este patrón se usa mucho para obtener dimensiones o valores únicos.\n",
    "\n",
    "### Extra: Selección por posición o muestra aleatoria\n",
    "\n",
    "A diferencia de Pandas, no tenemos `.iloc` o `.loc` porque el DataFrame no está ordenado intrínsecamente (y está distribuido). Si necesitamos una muestra de filas, podemos usar `df.limit(n)` para las primeras *n* filas (orden de aparición no definido a menos que se aplique un sort previo) o `df.sample(fraction=0.1)` para una fracción aleatoria. También existe `df.sampleBy(col, fractions={valor: prob, ...})` para muestreo estratificado según una columna categórica.\n",
    "\n",
    "Por ejemplo, `df.sample(False, 0.1).show()` daría \\~10% de las filas aleatoriamente (el primer argumento indica si es con reemplazo o no, casi siempre False).\n",
    "\n",
    "En la práctica, para *ver* datos es mejor usar `limit` después de ordenar, para tener una vista consistente, o simplemente usar `show()` que ya limita.\n",
    "\n",
    "Ahora que sabemos seleccionar columnas y filtrar filas, pasemos a operaciones más complejas: **agrupamientos y agregaciones**.\n",
    "\n",
    "## 4. Operaciones de agrupamiento (GroupBy y agregaciones)\n",
    "\n",
    "Agrupar datos por alguna clave y aplicar funciones de agregación (suma, promedio, conteo, etc.) es otra tarea esencial en análisis de datos, equivalente al `GROUP BY` de SQL o `groupby` de Pandas. PySpark proporciona la operación `groupBy` en DataFrames que nos devuelve un objeto de agrupamiento sobre el cual podemos aplicar agregaciones.\n",
    "\n",
    "La interfaz es ligeramente distinta a Pandas: en Pandas haríamos `df.groupby(\"col\").agg({\"otra_col\":\"mean\"})` por ejemplo. En PySpark podemos hacer:\n",
    "\n",
    "```python\n",
    "df.groupBy(\"species\").agg({\"sepal_length\": \"avg\"}).show()\n",
    "```\n",
    "\n",
    "Pero suele ser más claro usar las funciones de agregación de `pyspark.sql.functions` explicítamente, por ejemplo:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import avg, count, max, min\n",
    "\n",
    "df.groupBy(\"species\") \\\n",
    "  .agg(\n",
    "      count(\"*\").alias(\"count\"),\n",
    "      avg(\"sepal_length\").alias(\"avg_sepal_length\"),\n",
    "      avg(\"sepal_width\").alias(\"avg_sepal_width\")\n",
    "  ) \\\n",
    "  .show()\n",
    "```\n",
    "\n",
    "Supongamos que ejecutamos lo anterior en el dataset iris (que tiene 150 filas, 50 por especie). Obtendríamos algo como:\n",
    "\n",
    "```\n",
    "+----------+-----+----------------+---------------+\n",
    "|   species|count|avg_sepal_length|avg_sepal_width|\n",
    "+----------+-----+----------------+---------------+\n",
    "|    setosa|   50|            5.01|           3.42|\n",
    "|virginica|   50|            6.59|           2.97|\n",
    "|versicolor|   50|            5.94|           2.77|\n",
    "+----------+-----+----------------+---------------+\n",
    "```\n",
    "\n",
    "Aquí vemos 3 grupos (las especies únicas) con el tamaño de cada grupo y los promedios de longitud y anchura de sépalo en cada especie.\n",
    "\n",
    "Veamos otro ejemplo de agrupamiento: usando un dataset ficticio de ventas, podríamos agrupar por país y año para sumar ingresos, etc. En el tutorial de Spark de Datacamp, por ejemplo, agrupan por país para contar clientes distintos:\n",
    "\n",
    "```python\n",
    "# (ejemplo adaptado)\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df_sales.groupBy(\"Country\") \\\n",
    "    .agg(countDistinct(\"CustomerID\").alias(\"num_customers\")) \\\n",
    "    .orderBy(\"num_customers\", ascending=False) \\\n",
    "    .show()\n",
    "```\n",
    "\n",
    "Esto daría el número de clientes por país, ordenados de mayor a menor. La función `countDistinct` (o su alias `countDistinct`) es equivalente a COUNT(DISTINCT col) en SQL.\n",
    "\n",
    "Algunas **funciones de agregación comunes** disponibles (en `pyspark.sql.functions`): `count()`, `countDistinct()`, `sum()`, `avg()`/`mean()`, `min()`, `max()`, `stddev()`, `first()`, `last()`, `collect_list()` (recoger todos los valores en una lista), entre muchas otras. Puedes consultar la documentación de PySpark para la lista completa. También es posible definir **agregaciones personalizadas** (aunque es más avanzado, con `UserDefinedAggregateFunction` o usando funciones de ventana).\n",
    "\n",
    "**Atajo**: Si quisieras hacer simplemente una agregación global (no groupBy, sino sobre todo el DataFrame), puedes usar métodos como `df.count()` (cuenta total de filas), o `df.agg(max(\"col\"), min(\"col2\")).show()`, etc. Por ejemplo, `df_sales.agg({\"Revenue\": \"sum\"}).show()` te da la suma de la columna Revenue en todo el DF.\n",
    "\n",
    "**GroupBy vs SQL**: Todo lo que hacemos con groupBy se puede hacer también registrando el DataFrame como una tabla temporal y lanzando una consulta SQL. Por ejemplo:\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"iris_table\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT species, COUNT(*) as count,\n",
    "           AVG(sepal_length) as avg_sepal_length\n",
    "    FROM iris_table\n",
    "    GROUP BY species\n",
    "    \"\"\").show()\n",
    "```\n",
    "\n",
    "Esto produciría el mismo resultado que el groupBy anterior, pero escrito en SQL. Úsalo si te sientes más cómodo con SQL o si quieres aprovechar sintaxis complejas de SQL (Spark SQL soporta bastante de ANSI SQL, funciones ventana, etc.). Recuerda que la tabla temporal solo vive en esa SparkSession.\n",
    "\n",
    "### Ejemplo de agrupación práctica\n",
    "\n",
    "Tomemos nuestro DataFrame `df` de iris. Podemos preguntar: ¿cuál es el *ancho* medio de pétalo por cada especie, y cuántas muestras hay de cada una?:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "result = df.groupBy(\"species\") \\\n",
    "           .agg(\n",
    "               count(\"*\").alias(\"count\"),\n",
    "               round(avg(\"petal_width\"), 2).alias(\"avg_petal_width\")\n",
    "           )\n",
    "result.show()\n",
    "# +----------+-----+---------------+\n",
    "# |   species|count|avg_petal_width|\n",
    "# +----------+-----+---------------+\n",
    "# |    setosa|   50|           0.25|\n",
    "# |virginica|   50|           2.03|\n",
    "# |versicolor|   50|           1.33|\n",
    "# +----------+-----+---------------+\n",
    "```\n",
    "\n",
    "Aquí incluimos `round` para limitar a 2 decimales el promedio de ancho de pétalo (solo por presentación). Vemos que setosa tiene 50 muestras con ancho medio \\~0.25, etc.\n",
    "\n",
    "**Ordenar los resultados agrupados:** Muchas veces, tras agrupar queremos ordenar por el valor agregado. En Spark, simplemente encadenamos un `.orderBy()` o `.sort()` después de la agregación, como se ve en el ejemplo con num\\_customers arriba. Por ejemplo, `result.orderBy(\"avg_petal_width\", ascending=False).show()` ordenaría las especies del ancho de pétalo promedio mayor al menor (virginica primero en este caso).\n",
    "\n",
    "**Filtrar después de agrupar:** ¿Y si quisiéramos, por ejemplo, filtrar grupos por alguna condición agregada (como un HAVING en SQL)? Una forma es calcular la agregación y luego filtrar el DataFrame resultante. Otra forma es usar las funciones de agregación en una expresión filter: Spark 3.4 introdujo `.filter` después de groupBy (pandas API style), pero la manera más clara es hacer el groupBy.agg en un DF y luego filtrar sobre él:\n",
    "\n",
    "```python\n",
    "result.filter(result[\"count\"] > 40).show()\n",
    "```\n",
    "\n",
    "Esto mostraría solo las especies con más de 40 muestras (que son todas en iris). En SQL habríamos escrito `HAVING count(*) > 40`.\n",
    "\n",
    "En resumen, agrupar en PySpark es muy poderoso y similar a SQL. **Comparación con Pandas**: la principal diferencia es sintaxis. En Pandas podrías hacer `df.groupby(\"species\")[\"petal_width\"].mean()`, que devuelve una Series, etc. En PySpark, cada agregación devuelve un nuevo DataFrame con las columnas de grupo más las agregadas. No hay un concepto de *GroupBy object* que se imprima por sí solo; siempre necesitas una agregación final para obtener resultados. Además, las funciones disponibles cubren lo típico; para cosas como mediana o percentiles, hay funciones especiales (`approxQuantile` para estimar percentiles, o usar expr SQL).\n",
    "\n",
    "## 5. Otras operaciones avanzadas en PySpark DataFrames\n",
    "\n",
    "En esta sección revisaremos otras operaciones comunes que no hemos cubierto pero que son típicas al trabajar con datos, comparando con cómo se haría en Pandas cuando aplique. Incluiremos: uniones (joins) entre DataFrames, ordenamiento (*sorting*), añadir o transformar columnas, manejo de datos faltantes, y algunas consideraciones de rendimiento.\n",
    "\n",
    "### 5.1 Joins (unir DataFrames por una clave)\n",
    "\n",
    "La combinación de conjuntos de datos por una clave es soportada en Spark de forma similar a SQL y Pandas (merge). Podemos usar tanto la API DataFrame como expresiones SQL para hacer *joins*. Supongamos que tenemos dos DataFrames: `df_people` con columnas `persona_id, nombre, edad`, y `df_ciudad` con `ciudad_id, nombre_ciudad, pais`. Si quisiéramos unir la información de la persona con la de su ciudad natal (asumiendo que `df_people` tiene una columna `ciudad_id`), haríamos:\n",
    "\n",
    "```python\n",
    "df_people.join(df_ciudad, on=\"ciudad_id\", how=\"inner\").show()\n",
    "```\n",
    "\n",
    "Aquí Spark realizará por defecto un *inner join* (equivalente a `how=\"inner\"` explícito) usando la columna común `ciudad_id`. Podemos especificar tipos de join: `\"inner\"`, `\"left\"`, `\"right\"`, `\"outer\"` (full outer), `\"left_semi\"` (semi join), `\"left_anti\"` (anti join, útil para encontrar registros en A que no tienen correspondencia en B). Si las columnas llave tienen distinto nombre, debemos usar la sintaxis con expresión de condición:\n",
    "\n",
    "```python\n",
    "dfA.join(dfB, dfA.key == dfB.otherKey, how=\"left\")\n",
    "```\n",
    "\n",
    "o incluso múltiples condiciones encadenadas con `&`.\n",
    "\n",
    "Otra opción es renombrar previamente alguna columna para que coincidan los nombres y usar el parámetro `on=[\"colNameA\",\"colNameB\"]`.\n",
    "\n",
    "**Ejemplo práctico de join:** Volvamos al dataset iris. Supongamos que tenemos un segundo DataFrame que da información adicional de cada especie, por ejemplo:\n",
    "\n",
    "```python\n",
    "info = [(\"setosa\", \"color azul\"), \n",
    "        (\"versicolor\", \"color amarillo\"), \n",
    "        (\"virginica\", \"color rojo\")]\n",
    "df_info = spark.createDataFrame(info, [\"species\", \"flower_color\"])\n",
    "df_info.show()\n",
    "# +----------+--------------+\n",
    "# |   species|  flower_color|\n",
    "# +----------+--------------+\n",
    "# |    setosa|    color azul|\n",
    "# | versicolor|color amarillo|\n",
    "# | virginica|     color rojo|\n",
    "# +----------+--------------+\n",
    "```\n",
    "\n",
    "Ahora queremos agregar la columna `flower_color` al DataFrame principal `df` uniendo por `species`. Hacemos:\n",
    "\n",
    "```python\n",
    "df_joined = df.join(df_info, on=\"species\", how=\"inner\")\n",
    "df_joined.select(\"sepal_length\",\"sepal_width\",\"species\",\"flower_color\").show(5)\n",
    "# +------------+-----------+-------+--------------+\n",
    "# |sepal_length|sepal_width|species|  flower_color|\n",
    "# +------------+-----------+-------+--------------+\n",
    "# |         5.1|        3.5| setosa|    color azul|\n",
    "# |         4.9|        3.0| setosa|    color azul|\n",
    "# |         4.7|        3.2| setosa|    color azul|\n",
    "# |         4.6|        3.1| setosa|    color azul|\n",
    "# |         5.0|        3.6| setosa|    color azul|\n",
    "# +------------+-----------+-------+--------------+\n",
    "```\n",
    "\n",
    "Vemos que se añadió correctamente la información del color para cada fila según la especie. Un detalle: si el DataFrame de la derecha tuviera columnas con el mismo nombre que las de la izquierda (aparte de la llave), Spark las renombraría agregando sufijos (`_right`) automáticamente para diferenciar. Podemos evitar conflictos seleccionando solo columnas necesarias o renombrando antes del join.\n",
    "\n",
    "**Joins usando SQL**: Alternativamente podríamos hacer `spark.sql(\"SELECT ... FROM df LEFT JOIN otherdf ...\")` tras crear vistas temporales, como se mostró antes. Spark SQL soporta las sintaxis JOIN estándar.\n",
    "\n",
    "**Broadcast join (tip avanzado)**: Si uno de los DataFrames es pequeño (p. ej., una tabla de dimensión) y el otro muy grande, Spark automáticamente o manualmente puede hacer un *broadcast join*, enviando la tabla pequeña a cada nodo en lugar de hacer shuffle de la grande. Si Spark no lo hace por sí solo, se puede forzar con `broadcast(df_small)` en la expresión de join (importando `from pyspark.sql.functions import broadcast`). Esto optimiza mucho los joins tipo dimensión.\n",
    "\n",
    "### 5.2 Ordenamiento (sort)\n",
    "\n",
    "Ordenar un DataFrame por valores de columna es similar a SQL (`ORDER BY`) o Pandas (`sort_values`). En PySpark usamos `.orderBy()` o su sinónimo `.sort()`. Ejemplos:\n",
    "\n",
    "```python\n",
    "# Ordenar iris por sepal_length ascendentemente:\n",
    "df.orderBy(\"sepal_length\", ascending=True).show(5)\n",
    "\n",
    "# Ordenar descendentemente, dos formas equivalentes:\n",
    "from pyspark.sql.functions import desc\n",
    "df.orderBy(desc(\"sepal_length\")).show(5)\n",
    "\n",
    "df.sort(df.sepal_length.desc()).show(5)\n",
    "```\n",
    "\n",
    "También podemos ordenar por múltiples columnas, por ejemplo primero por especie alfabéticamente y luego por sepal\\_length desc:\n",
    "\n",
    "```python\n",
    "df.sort(\"species\", df.sepal_length.desc()).show(5)\n",
    "```\n",
    "\n",
    "En el contenido de ejemplo, muestran varias maneras y confirmación de que `sort` y `orderBy` son lo mismo. También destacan que podemos pasar una lista de columnas y un listado booleano para órdenes diferentes, por ejemplo:\n",
    "\n",
    "```python\n",
    "df.sort([\"species\",\"sepal_length\"], ascending=[1, 0]).show(5)\n",
    "```\n",
    "\n",
    "Aquí 1 = ascendente, 0 = descendente.\n",
    "\n",
    "**Limit (Top-N):** Muy a menudo queremos los \"top N\" registros según algún criterio. En Pandas harías sort\\_values y head(n). En PySpark, podríamos usar `.orderBy(...).limit(n)` para dejar que la ordenación ocurra a nivel distribuido pero solo recoger las primeras n filas. Por ejemplo:\n",
    "\n",
    "```python\n",
    "# Top 5 flores con mayor longitud de pétalo\n",
    "df.orderBy(df.petal_length.desc()).limit(5).show()\n",
    "```\n",
    "\n",
    "Spark intentará no mover todos los datos al driver, sino solo las filas necesarias, haciendo la operación de forma más eficiente que un collect completo.\n",
    "\n",
    "### 5.3 Añadir o transformar columnas (features engineering)\n",
    "\n",
    "Para crear nuevas columnas derivadas de existentes (lo que en Pandas haríamos con `df[\"nueva\"] = ...`), en PySpark usamos `.withColumn(nombre, expr)`. Ya vimos un ejemplo en la sección 2: para convertir el tipo de una columna, también se usa withColumn (sobreescribiendo la existente).\n",
    "\n",
    "Ejemplo: queremos calcular el área del sépalo (suponiendo rectángulo: sepal\\_length \\* sepal\\_width) en nuestro DataFrame iris:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"sepal_area\", df.sepal_length * df.sepal_width)\n",
    "df.select(\"sepal_length\",\"sepal_width\",\"sepal_area\").show(5)\n",
    "# +------------+-----------+----------+\n",
    "# |sepal_length|sepal_width|sepal_area|\n",
    "# +------------+-----------+----------+\n",
    "# |         5.1|        3.5|      17.85|\n",
    "# |         4.9|        3.0|      14.70|\n",
    "# |         4.7|        3.2|      15.04|\n",
    "# |         4.6|        3.1|      14.26|\n",
    "# |         5.0|        3.6|      18.00|\n",
    "# +------------+-----------+----------+\n",
    "```\n",
    "\n",
    "Hemos agregado la columna *sepal\\_area*. Internamente, con withColumn Spark construye una nueva DataFrame agregando esa columna; recuerden reasignar al mismo `df` o a otro variable, ya que el DataFrame original sigue sin esa columna si no lo reasignamos (inmutabilidad).\n",
    "\n",
    "También podemos usar muchas funciones de la librería `pyspark.sql.functions` para crear columnas: matemáticas (`+`, `-`, `*`, etc. funcionan directo como vimos), funciones de cadena, fechas, condicionales (`when`), etc. Un ejemplo: supongamos que queremos una columna categórica basada en ancho de pétalo, digamos \"petal\\_size\" que sea \"largo\" si petal\\_length > 4, sino \"corto\". Podemos usar `when`:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import when\n",
    "df = df.withColumn(\"petal_size\",\n",
    "                   when(df.petal_length > 4.0, \"largo\").otherwise(\"corto\"))\n",
    "df.select(\"petal_length\",\"petal_size\").show(8)\n",
    "# +------------+----------+\n",
    "# |petal_length|petal_size|\n",
    "# +------------+----------+\n",
    "# |         1.4|     corto|\n",
    "# |         1.4|     corto|\n",
    "# |         1.3|     corto|\n",
    "# |         1.5|     corto|\n",
    "# |         1.4|     corto|\n",
    "# |         4.7|     largo|\n",
    "# |         4.5|     largo|\n",
    "# |         4.9|     largo|\n",
    "# +------------+----------+\n",
    "```\n",
    "\n",
    "Las primeras 5 eran setosa (pétalos pequeños) y marcaron \"corto\", luego versicolor/viginica aparecen como \"largo\" en nuestro ejemplo de muestra.\n",
    "\n",
    "**Reemplazar valores existentes:** Si usamos un nombre de columna que ya existe en withColumn, Spark la reemplaza por la nueva. Esto es útil para transformar tipos (como casteos, normalizaciones, etc.). Por ejemplo, `df = df.withColumn(\"edad\", df.edad.cast(\"int\"))` convertiría a entero la columna edad (antes string, por decir algo).\n",
    "\n",
    "**Renombrar columnas:** Con Pandas harías `df.rename(columns={\"old\":\"new\"})`. En PySpark, podemos usar `withColumnRenamed(\"oldName\",\"newName\")`. Esto devuelve un DataFrame con el cambio de nombre aplicado. Ojo: si necesitas renombrar múltiples columnas, hay que llamarlo varias veces o encadenado.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```python\n",
    "df = df.withColumnRenamed(\"sepal_length\", \"sepal_len\")\n",
    "```\n",
    "\n",
    "**Eliminar columnas:** Ya vimos `df.drop(\"col\")` para quitar columnas. Devuelve un nuevo DataFrame sin esas columnas. Ten en cuenta que drop no modifica el original hasta reasignar.\n",
    "\n",
    "### 5.4 Manejo de valores nulos y datos faltantes\n",
    "\n",
    "Ya comentamos brevemente dropna y fillna. Para recapitular:\n",
    "\n",
    "* `df.na.drop()` elimina filas con nulos (puedes especificar subset de columnas y umbral mínimo de valores no nulos).\n",
    "* `df.na.fill(value, subset=[\"col1\",\"col2\"])` rellena con un valor dado en las columnas indicadas los `NULL` que encuentre.\n",
    "\n",
    "Por ejemplo, si tuviéramos nulos en *sepal\\_width* y quisiéramos llenarlos con la media:\n",
    "\n",
    "```python\n",
    "mean_width = df.select(avg(\"sepal_width\")).first()[0]\n",
    "df_filled = df.na.fill(mean_width, subset=[\"sepal_width\"])\n",
    "```\n",
    "\n",
    "Spark también soporta imputación más avanzada vía MLlib (Imputer), pero eso es otro tema.\n",
    "\n",
    "En nuestro dataset iris no hay nulos, pero en la mayoría de casos del mundo real tendrás que lidiar con ellos.\n",
    "\n",
    "### 5.5 Uniones de filas (concat / union)\n",
    "\n",
    "Si queremos **apilar** dos DataFrames (mismo esquema) uno debajo del otro, en Pandas usaríamos `pd.concat([df1, df2])`. En PySpark se usa `df1.union(df2)`. Requiere que ambos DataFrames tengan exactamente el mismo esquema (mismos nombres de columnas y tipos, en el mismo orden). Si no, conviene renombrar/seleccionar previamente para alinearlos.\n",
    "\n",
    "Ejemplo sencillo: si dividimos nuestro `df` de iris en dos partes y luego queremos recombinarlas:\n",
    "\n",
    "```python\n",
    "df_part1 = df.limit(10)  # primeros 10\n",
    "df_part2 = df.filter(df.sepal_length > 5.0).limit(5)  # 5 cualquiera con sepal_length>5\n",
    "df_combined = df_part1.union(df_part2)\n",
    "print(df_combined.count())  # debería ser 15\n",
    "```\n",
    "\n",
    "Ten presente que `union` en Spark **no quita duplicados** (sería más parecido a UNION ALL en SQL). Para eliminar duplicados tras unir, puedes encadenar `.distinct()`.\n",
    "\n",
    "Existe también `unionByName` en PySpark, que une por nombre de columna en vez de posición (útil si las columnas están en distinto orden en los DataFrames).\n",
    "\n",
    "### 5.6 UDFs (User-Defined Functions) y funciones integradas\n",
    "\n",
    "Una tentación común al venir de Pandas es aplicar funciones Python arbitrarias a las filas (con `apply` por ejemplo). En PySpark, esto se puede hacer definiendo UDFs, pero es **desaconsejable** a menos que sea absolutamente necesario, porque las UDF en Python no se optimizan y pueden ralentizar muchísimo el cómputo (Spark tendrá que serializar/deserializar datos para cada fila, perdiendo vectorización y optimizaciones).\n",
    "\n",
    "Siempre que sea posible, intenta usar las funciones integradas de Spark (las del módulo `pyspark.sql.functions`) que están en Scala/Java y son eficientes. Hay muchísimas: matemáticas, texto (expresiones regulares, substring, etc.), fechas (conversión a timestamp, datediff, etc.), arrays, JSON, etc. Si necesitas lógica condicional, combina `when`/`otherwise`. Si necesitas algo como mapear valores, considera usar expresiones como `when` múltiples o una join con tabla de mapeo en vez de un dict Python.\n",
    "\n",
    "Solo si tu lógica es demasiado específica y no existe equivalente Spark, recurre a UDF. Un UDF básico se define así:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def mi_funcion(x):\n",
    "    # ... hacer algo con x ...\n",
    "    return resultado\n",
    "\n",
    "# registrar la UDF informando tipo de retorno\n",
    "mi_udf = udf(mi_funcion, IntegerType())\n",
    "\n",
    "df = df.withColumn(\"nueva_col\", mi_udf(df.existing_col))\n",
    "```\n",
    "\n",
    "Spark enviará la función `mi_funcion` a cada nodo, ejecutándola en cada fila de la partición. Nuevamente, **evita UDFs si puedes**. A partir de Spark 3.0, existen las *Pandas UDF* (a.k.a vectorized UDF) que permiten aplicar funciones Python a *pandas.Series* por partición en lugar de fila a fila, mejorando el rendimiento al aprovechar Apache Arrow. Esto está genial para ciertas tareas numéricas o científicas. Pero es un tema avanzado; solo mencionarlo como opción.\n",
    "\n",
    "### 5.7 Consideraciones de rendimiento y *pitfalls*\n",
    "\n",
    "Para concluir esta sección avanzada, resumamos algunos **tips y estrategias** al trabajar con PySpark:\n",
    "\n",
    "* **Evita traer datos completos al driver**: Operaciones como `collect()` o convertir a Pandas (`toPandas()`) traerán todos los datos al nodo maestro (driver). Úsalas solo si estás seguro de que el resultado cabe en memoria local. De lo contrario, puedes quedarse sin memoria o congelar la aplicación. En su lugar, intenta realizar la agregación o filtrado en el cluster y traer solo el resultado necesario (por ejemplo, en lugar de `df.collect()` para analizar en Python, quizá puedas hacer `df.filter(...).count()` en Spark o `take(n)` para unos pocos registros).\n",
    "* **Persistencia/caching**: Debido a la evaluación *lazy*, si vas a reutilizar el mismo DataFrame en múltiples operaciones o iteraciones, considera **cachar** el DataFrame en memoria con `df.cache()` o `df.persist()` después de su cálculo inicial. Por ejemplo, si vas a computar varias agregaciones diferentes sobre el mismo conjunto filtrado, cachear ese subconjunto evitará recomputarlo desde cero cada vez. Recuerda ejecutar una acción tras cachear para materializar los datos en memoria (p.ej. `df_cached = df.filter(...).cache(); df_cached.count()`).\n",
    "* **Particiones**: Spark divide los datos en particiones distribuidas. Un número muy bajo o muy alto de particiones puede afectar rendimiento. La regla general: unas \\~200MB por partición es eficiente. Puedes ver el número de particiones de un DF con `df.rdd.getNumPartitions()`. Para ajustar, usa `df.repartition(n)` (cambia a n particiones, barajando datos) o `df.coalesce(n)` (reduce particiones sin mover tantos datos, solo para achicar). Esto es útil, por ejemplo, después de un filtrado grande que dejó pocas filas en muchas particiones, se podría coalesce para reducir el overhead.\n",
    "* **Joins con datos desbalanceados**: Si un join produce un *shuffle* muy grande (reordenamiento de datos en la red), puede ser costoso. Considera usar *broadcast join* para tablas pequeñas como mencionamos, o ajustar la clave de partición si usas DataFrames con `partitionBy` en escritura/lectura.\n",
    "* **Ordenamientos costosos**: Un `orderBy` necesita mover y comparar muchos datos. Si solo te interesa el top N, usar `limit` después de order ayuda, pero para conjuntos masivos quizá explorar técnicas de approximate topN (Spark no lo da nativo, salvo usando RDDs). También, si los datos ya estaban ordenados por algún criterio (por diseño o tras lectura de un parquet particionado), podrías aprovecharlo. En general, acepta que sort es pesado en big data.\n",
    "* **Uso de la interfaz Spark SQL vs DataFrame**: Internamente ambas van al mismo motor. Usar `spark.sql` no es menos eficiente que la API DataFrame, es más bien preferencia. Combina según convenga; por ejemplo, a veces construir cierta lógica en Python es complicado y más fácil de expresar en SQL.\n",
    "* **Logging y UI**: Aprovecha el **Spark UI (puerto 4040)** mientras corres trabajos, para ver cuántas tareas se lanzan, tiempos, si hay *slots* desocupados, etc. Esto ayuda a detectar cuellos de botella (por ejemplo, una etapa de shuffle enorme). También puedes activar logs del plan lógico/físico si quieres entender qué hace Spark (con `df.explain()` por ejemplo, que imprime el plan de ejecución optimizado).\n",
    "\n",
    "Siguiendo estos consejos, podrás evitar algunos dolores de cabeza comunes al pasar de un entorno local (Pandas/SQL tradicional) a un entorno distribuido con PySpark.\n",
    "\n",
    "## 6. Integración con Pandas y uso del API pandas-on-Spark\n",
    "\n",
    "Hemos mencionado anteriormente la precaución de no convertir datos masivos a Pandas. Sin embargo, Pandas sigue siendo muy útil para volúmenes pequeños o para ciertas tareas como visualizaciones. Spark nos ofrece varias formas de integrarse con Pandas:\n",
    "\n",
    "### 6.1 Conversiones entre Spark DataFrame y Pandas DataFrame\n",
    "\n",
    "La forma más directa es usar los métodos de conversión:\n",
    "\n",
    "* **De Spark a Pandas**: `df.toPandas()` – Colecciona todo el DataFrame de Spark al driver y lo convierte en un `pandas.DataFrame`. **Úsalo solo cuando estés seguro de que el resultado cabe en memoria local**, porque esta operación traerá todos los datos al nodo maestro, rompiendo la distribución. Por ejemplo, después de una agregación fuerte que redujo mucho el dataset, o para tomar una muestra pequeña con la cual hacer un gráfico local con Matplotlib/Seaborn.\n",
    "\n",
    "* **De Pandas a Spark**: `spark.createDataFrame(pandas_df)` – ya lo usamos; envía los datos de pandas al cluster (vía Arrow) y crea un DataFrame distribuido.\n",
    "\n",
    "Ejemplo de flujo: supongamos que tras procesar nuestros datos en Spark, obtenemos un DataFrame `top_flowers_df` con los 10 tipos de flores más raros (imaginemos) con algunas métricas calculadas, y queremos hacer un gráfico de barras con Seaborn. Podríamos:\n",
    "\n",
    "```python\n",
    "pd_top = top_flowers_df.toPandas()  # pequeño, digamos 10 filas\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"flower_type\", y=\"rarity_score\", data=pd_top)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Esto es válido. Pero **cuidado**: al llamar `toPandas()`, si accidentalmente tu DataFrame era grande, podrías quedarte sin memoria en el driver. En el ejemplo del curso IABD, enfatizan esto mismo: al convertir a Pandas perdemos la distribución y nos arriesgamos a un *OutOfMemory*. Recomiendan hacerlo solo para **visualización o análisis rápido de un subconjunto pequeño**. En su caso convierten a Pandas un resultado resumido (10 ciudades con más votos, del dataset de Yelp) para graficar con Seaborn.\n",
    "\n",
    "De hecho, ellos dicen: *\"Mucho cuidado al utilizar Pandas, ya que al convertir el DataFrame nos vamos a traer todos los datos al driver, perdiendo la distribución y pudiendo provocar un error de falta de memoria. Hay que evitar a toda costa utilizar Pandas para tratar los datos, ya que perdemos toda la potencia de trabajo en clúster (Pandas solo puede usar los recursos del nodo principal). Únicamente lo utilizaremos cuando vayamos a visualizar datos mediante Matplotlib/Seaborn...\"*. Esto resume perfectamente la filosofía: Pandas solo al final, en porciones manejables.\n",
    "\n",
    "### 6.2 pandas-on-Spark (API de Pandas sobre Spark)\n",
    "\n",
    "Una novedad muy interesante es que desde Spark 3.2 en adelante se integró el proyecto **Koalas** dentro de PySpark, renombrado como *Pandas API on Spark*. Esto permite usar una sintaxis *muy similar a Pandas* pero ejecutando en Spark por detrás. La idea es ayudar a quienes conocen Pandas a aprovechar Spark sin aprender toda la API nueva.\n",
    "\n",
    "Para usarlo, se importa:\n",
    "\n",
    "```python\n",
    "import pyspark.pandas as ps\n",
    "```\n",
    "\n",
    "y a partir de ahí, `ps.DataFrame` y `ps.Series` son equivalentes a pandas.DataFrame/Series pero operados por Spark en el backend. Por ejemplo, podrías cargar un CSV directamente con `ps.read_csv(\"file.csv\")`, obtener un `ps.DataFrame` (que internamente es un DataFrame Spark particionado), y luego usar `df_ps[df_ps[\"col\"] > 5]`, `df_ps.groupby(\"x\").y.mean()` tal como lo harías en pandas. *¡Ojo!* que debajo sigue aplicando *lazy evaluation* y ejecutándose en el cluster, no todo es inmediato como Pandas. Pero la sintaxis es mucho más parecida.\n",
    "\n",
    "Un mini-ejemplo:\n",
    "\n",
    "```python\n",
    "psdf = ps.DataFrame({\"x\": [1, 2, 3, 4], \"y\": [10, 20, 30, 40]})\n",
    "print(type(psdf))  # <class 'pyspark.pandas.frame.DataFrame'>\n",
    "psdf[\"z\"] = psdf[\"x\"] + psdf[\"y\"]  # similar a pandas syntax\n",
    "print(psdf.head(3))\n",
    "#    x   y   z\n",
    "# 0  1  10  11\n",
    "# 1  2  20  22\n",
    "# 2  3  30  33\n",
    "```\n",
    "\n",
    "Bajo el capó, las operaciones se traducen a operaciones Spark. Esta API está en evolución, cubre gran parte de las funcionalidades de Pandas (no el 100%, pero sí muchas). Es muy útil para hacer porting de código Pandas existente a Spark rápidamente. Sin embargo, hay que entender que no siempre tendrá el mismo rendimiento que usar directamente la API de Spark SQL (en algunos casos puede haber sobrecarga).\n",
    "\n",
    "En cualquier caso, es bueno saber que existe. En el futuro, quizá no sea necesario aprender tanto la API de Spark DataFrame porque *pandas-on-Spark* la abstraerá. Aún así, bajo el capó los conceptos de particiones, lazy eval, etc., siguen aplicando.\n",
    "\n",
    "Para cerrar, ten en cuenta:\n",
    "\n",
    "* Puedes **combinar** el uso de Spark y Pandas según la porción del workflow: Spark para la ingestión y transformaciones pesadas, Pandas cuando ya reduces a algo manejable o necesitas una librería específica que no tiene equivalente distribuido.\n",
    "* Siempre verifica dos veces los tamaños antes de hacer `toPandas()`. Un enfoque común es usar `df.limit(n).toPandas()` para traer solo n filas de muestra en lugar de todo.\n",
    "* La integración va más allá: Spark 3.0+ soporta UDFs vectorizadas con Pandas (como mencionamos) y también conversión eficiente vía Arrow. Asegúrate de tener instalado PyArrow en tu entorno para aprovechar esto (PySpark suele traerlo o lo recomienda, y lo activa con `spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")` si no viene ya por defecto).\n",
    "\n",
    "**Conclusión:** PySpark nos provee una plataforma potente para trabajar con big data usando Python. Hemos repasado desde la instalación en Docker, la creación de DataFrames, selección y filtrado, agrupaciones, joins, hasta tips avanzados y cómo integrarlo con herramientas familiares como Pandas. Con esta base, deberías poder comenzar a utilizar PySpark en tus propios datos, aprovechando tus conocimientos de Pandas/SQL pero teniendo siempre en mente las diferencias clave de un entorno distribuido. ¡Manos a la obra con PySpark!\n",
    "\n",
    "**Referencias seleccionadas:**\n",
    "\n",
    "* Documentación oficial de Apache Spark: **Spark SQL, DataFrames and Datasets Guide** (guía completa de uso de DataFrames y SQL en Spark).\n",
    "* Libro *Beginning Apache Spark 3* (Stanley & Zanoran, 2021).\n",
    "* Sitio web **Spark by Examples** – múltiples ejemplos prácticos de funciones PySpark.\n",
    "* Artículo *The Most Complete Guide to PySpark DataFrames* en TowardsDataScience.\n",
    "* Documentación de la API Pandas en Spark (PySpark), y artículo *Migrating from Koalas to pandas API on Spark* (Apache Spark official).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
