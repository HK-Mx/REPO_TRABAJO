{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17b20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d70842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353949d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [3, 5, 10],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 3, 4, 5],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [100, 50, 200]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=10, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [3, 5, 10],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 3, 4, 5],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [100, 50, 200]})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: RandomForestClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                         'max_depth': [3, 5, 10],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4, 5],\n",
       "                         'min_samples_split': [2, 3, 4, 5],\n",
       "                         'n_estimators': [100, 50, 200]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"n_estimators\" : [100,50,200],\n",
    "    \"max_depth\":[3,5,10],\n",
    "    \"min_samples_split\":[2,3,4,5],\n",
    "    \"min_samples_leaf\":[1,2,3,4,5],\n",
    "}\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "clf = GridSearchCV(estimator = rfc,\n",
    "                    param_grid = parameters,\n",
    "                    n_jobs = -1,\n",
    "                    cv = 10)\n",
    "\n",
    "clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db02e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=3, n_estimators=50)\n",
      "1\n",
      "{'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "0.9666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_estimator_)\n",
    "print(clf.best_index_)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "666f3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93baa208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "RandomForestClassifier(\n",
      "    n_estimators=\u001b[32m100\u001b[39m,\n",
      "    *,\n",
      "    criterion=\u001b[33m'gini'\u001b[39m,\n",
      "    max_depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    min_samples_split=\u001b[32m2\u001b[39m,\n",
      "    min_samples_leaf=\u001b[32m1\u001b[39m,\n",
      "    min_weight_fraction_leaf=\u001b[32m0.0\u001b[39m,\n",
      "    max_features=\u001b[33m'sqrt'\u001b[39m,\n",
      "    max_leaf_nodes=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    min_impurity_decrease=\u001b[32m0.0\u001b[39m,\n",
      "    bootstrap=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    oob_score=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    verbose=\u001b[32m0\u001b[39m,\n",
      "    warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    class_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ccp_alpha=\u001b[32m0.0\u001b[39m,\n",
      "    max_samples=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    monotonic_cst=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m RandomForestClassifier(ForestClassifier):\n",
      "    \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m    A random forest classifier.\u001b[39m\n",
      "\n",
      "\u001b[33m    A random forest is a meta estimator that fits a number of decision tree\u001b[39m\n",
      "\u001b[33m    classifiers on various sub-samples of the dataset and uses averaging to\u001b[39m\n",
      "\u001b[33m    improve the predictive accuracy and control over-fitting.\u001b[39m\n",
      "\u001b[33m    Trees in the forest use the best split strategy, i.e. equivalent to passing\u001b[39m\n",
      "\u001b[33m    `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeClassifier`.\u001b[39m\n",
      "\u001b[33m    The sub-sample size is controlled with the `max_samples` parameter if\u001b[39m\n",
      "\u001b[33m    `bootstrap=True` (default), otherwise the whole dataset is used to build\u001b[39m\n",
      "\u001b[33m    each tree.\u001b[39m\n",
      "\n",
      "\u001b[33m    For a comparison between tree-based ensemble models see the example\u001b[39m\n",
      "\u001b[33m    :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\u001b[39m\n",
      "\n",
      "\u001b[33m    Read more in the :ref:`User Guide <forest>`.\u001b[39m\n",
      "\n",
      "\u001b[33m    Parameters\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    n_estimators : int, default=100\u001b[39m\n",
      "\u001b[33m        The number of trees in the forest.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionchanged:: 0.22\u001b[39m\n",
      "\u001b[33m           The default value of ``n_estimators`` changed from 10 to 100\u001b[39m\n",
      "\u001b[33m           in 0.22.\u001b[39m\n",
      "\n",
      "\u001b[33m    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\u001b[39m\n",
      "\u001b[33m        The function to measure the quality of a split. Supported criteria are\u001b[39m\n",
      "\u001b[33m        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\u001b[39m\n",
      "\u001b[33m        Shannon information gain, see :ref:`tree_mathematical_formulation`.\u001b[39m\n",
      "\u001b[33m        Note: This parameter is tree-specific.\u001b[39m\n",
      "\n",
      "\u001b[33m    max_depth : int, default=None\u001b[39m\n",
      "\u001b[33m        The maximum depth of the tree. If None, then nodes are expanded until\u001b[39m\n",
      "\u001b[33m        all leaves are pure or until all leaves contain less than\u001b[39m\n",
      "\u001b[33m        min_samples_split samples.\u001b[39m\n",
      "\n",
      "\u001b[33m    min_samples_split : int or float, default=2\u001b[39m\n",
      "\u001b[33m        The minimum number of samples required to split an internal node:\u001b[39m\n",
      "\n",
      "\u001b[33m        - If int, then consider `min_samples_split` as the minimum number.\u001b[39m\n",
      "\u001b[33m        - If float, then `min_samples_split` is a fraction and\u001b[39m\n",
      "\u001b[33m          `ceil(min_samples_split * n_samples)` are the minimum\u001b[39m\n",
      "\u001b[33m          number of samples for each split.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionchanged:: 0.18\u001b[39m\n",
      "\u001b[33m           Added float values for fractions.\u001b[39m\n",
      "\n",
      "\u001b[33m    min_samples_leaf : int or float, default=1\u001b[39m\n",
      "\u001b[33m        The minimum number of samples required to be at a leaf node.\u001b[39m\n",
      "\u001b[33m        A split point at any depth will only be considered if it leaves at\u001b[39m\n",
      "\u001b[33m        least ``min_samples_leaf`` training samples in each of the left and\u001b[39m\n",
      "\u001b[33m        right branches.  This may have the effect of smoothing the model,\u001b[39m\n",
      "\u001b[33m        especially in regression.\u001b[39m\n",
      "\n",
      "\u001b[33m        - If int, then consider `min_samples_leaf` as the minimum number.\u001b[39m\n",
      "\u001b[33m        - If float, then `min_samples_leaf` is a fraction and\u001b[39m\n",
      "\u001b[33m          `ceil(min_samples_leaf * n_samples)` are the minimum\u001b[39m\n",
      "\u001b[33m          number of samples for each node.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionchanged:: 0.18\u001b[39m\n",
      "\u001b[33m           Added float values for fractions.\u001b[39m\n",
      "\n",
      "\u001b[33m    min_weight_fraction_leaf : float, default=0.0\u001b[39m\n",
      "\u001b[33m        The minimum weighted fraction of the sum total of weights (of all\u001b[39m\n",
      "\u001b[33m        the input samples) required to be at a leaf node. Samples have\u001b[39m\n",
      "\u001b[33m        equal weight when sample_weight is not provided.\u001b[39m\n",
      "\n",
      "\u001b[33m    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\u001b[39m\n",
      "\u001b[33m        The number of features to consider when looking for the best split:\u001b[39m\n",
      "\n",
      "\u001b[33m        - If int, then consider `max_features` features at each split.\u001b[39m\n",
      "\u001b[33m        - If float, then `max_features` is a fraction and\u001b[39m\n",
      "\u001b[33m          `max(1, int(max_features * n_features_in_))` features are considered at each\u001b[39m\n",
      "\u001b[33m          split.\u001b[39m\n",
      "\u001b[33m        - If \"sqrt\", then `max_features=sqrt(n_features)`.\u001b[39m\n",
      "\u001b[33m        - If \"log2\", then `max_features=log2(n_features)`.\u001b[39m\n",
      "\u001b[33m        - If None, then `max_features=n_features`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionchanged:: 1.1\u001b[39m\n",
      "\u001b[33m            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\u001b[39m\n",
      "\n",
      "\u001b[33m        Note: the search for a split does not stop until at least one\u001b[39m\n",
      "\u001b[33m        valid partition of the node samples is found, even if it requires to\u001b[39m\n",
      "\u001b[33m        effectively inspect more than ``max_features`` features.\u001b[39m\n",
      "\n",
      "\u001b[33m    max_leaf_nodes : int, default=None\u001b[39m\n",
      "\u001b[33m        Grow trees with ``max_leaf_nodes`` in best-first fashion.\u001b[39m\n",
      "\u001b[33m        Best nodes are defined as relative reduction in impurity.\u001b[39m\n",
      "\u001b[33m        If None then unlimited number of leaf nodes.\u001b[39m\n",
      "\n",
      "\u001b[33m    min_impurity_decrease : float, default=0.0\u001b[39m\n",
      "\u001b[33m        A node will be split if this split induces a decrease of the impurity\u001b[39m\n",
      "\u001b[33m        greater than or equal to this value.\u001b[39m\n",
      "\n",
      "\u001b[33m        The weighted impurity decrease equation is the following::\u001b[39m\n",
      "\n",
      "\u001b[33m            N_t / N * (impurity - N_t_R / N_t * right_impurity\u001b[39m\n",
      "\u001b[33m                                - N_t_L / N_t * left_impurity)\u001b[39m\n",
      "\n",
      "\u001b[33m        where ``N`` is the total number of samples, ``N_t`` is the number of\u001b[39m\n",
      "\u001b[33m        samples at the current node, ``N_t_L`` is the number of samples in the\u001b[39m\n",
      "\u001b[33m        left child, and ``N_t_R`` is the number of samples in the right child.\u001b[39m\n",
      "\n",
      "\u001b[33m        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\u001b[39m\n",
      "\u001b[33m        if ``sample_weight`` is passed.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.19\u001b[39m\n",
      "\n",
      "\u001b[33m    bootstrap : bool, default=True\u001b[39m\n",
      "\u001b[33m        Whether bootstrap samples are used when building trees. If False, the\u001b[39m\n",
      "\u001b[33m        whole dataset is used to build each tree.\u001b[39m\n",
      "\n",
      "\u001b[33m    oob_score : bool or callable, default=False\u001b[39m\n",
      "\u001b[33m        Whether to use out-of-bag samples to estimate the generalization score.\u001b[39m\n",
      "\u001b[33m        By default, :func:`~sklearn.metrics.accuracy_score` is used.\u001b[39m\n",
      "\u001b[33m        Provide a callable with signature `metric(y_true, y_pred)` to use a\u001b[39m\n",
      "\u001b[33m        custom metric. Only available if `bootstrap=True`.\u001b[39m\n",
      "\n",
      "\u001b[33m    n_jobs : int, default=None\u001b[39m\n",
      "\u001b[33m        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\u001b[39m\n",
      "\u001b[33m        :meth:`decision_path` and :meth:`apply` are all parallelized over the\u001b[39m\n",
      "\u001b[33m        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\u001b[39m\n",
      "\u001b[33m        context. ``-1`` means using all processors. See :term:`Glossary\u001b[39m\n",
      "\u001b[33m        <n_jobs>` for more details.\u001b[39m\n",
      "\n",
      "\u001b[33m    random_state : int, RandomState instance or None, default=None\u001b[39m\n",
      "\u001b[33m        Controls both the randomness of the bootstrapping of the samples used\u001b[39m\n",
      "\u001b[33m        when building trees (if ``bootstrap=True``) and the sampling of the\u001b[39m\n",
      "\u001b[33m        features to consider when looking for the best split at each node\u001b[39m\n",
      "\u001b[33m        (if ``max_features < n_features``).\u001b[39m\n",
      "\u001b[33m        See :term:`Glossary <random_state>` for details.\u001b[39m\n",
      "\n",
      "\u001b[33m    verbose : int, default=0\u001b[39m\n",
      "\u001b[33m        Controls the verbosity when fitting and predicting.\u001b[39m\n",
      "\n",
      "\u001b[33m    warm_start : bool, default=False\u001b[39m\n",
      "\u001b[33m        When set to ``True``, reuse the solution of the previous call to fit\u001b[39m\n",
      "\u001b[33m        and add more estimators to the ensemble, otherwise, just fit a whole\u001b[39m\n",
      "\u001b[33m        new forest. See :term:`Glossary <warm_start>` and\u001b[39m\n",
      "\u001b[33m        :ref:`tree_ensemble_warm_start` for details.\u001b[39m\n",
      "\n",
      "\u001b[33m    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\u001b[39m\n",
      "\u001b[33m            default=None\u001b[39m\n",
      "\u001b[33m        Weights associated with classes in the form ``{class_label: weight}``.\u001b[39m\n",
      "\u001b[33m        If not given, all classes are supposed to have weight one. For\u001b[39m\n",
      "\u001b[33m        multi-output problems, a list of dicts can be provided in the same\u001b[39m\n",
      "\u001b[33m        order as the columns of y.\u001b[39m\n",
      "\n",
      "\u001b[33m        Note that for multioutput (including multilabel) weights should be\u001b[39m\n",
      "\u001b[33m        defined for each class of every column in its own dict. For example,\u001b[39m\n",
      "\u001b[33m        for four-class multilabel classification weights should be\u001b[39m\n",
      "\u001b[33m        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\u001b[39m\n",
      "\u001b[33m        [{1:1}, {2:5}, {3:1}, {4:1}].\u001b[39m\n",
      "\n",
      "\u001b[33m        The \"balanced\" mode uses the values of y to automatically adjust\u001b[39m\n",
      "\u001b[33m        weights inversely proportional to class frequencies in the input data\u001b[39m\n",
      "\u001b[33m        as ``n_samples / (n_classes * np.bincount(y))``\u001b[39m\n",
      "\n",
      "\u001b[33m        The \"balanced_subsample\" mode is the same as \"balanced\" except that\u001b[39m\n",
      "\u001b[33m        weights are computed based on the bootstrap sample for every tree\u001b[39m\n",
      "\u001b[33m        grown.\u001b[39m\n",
      "\n",
      "\u001b[33m        For multi-output, the weights of each column of y will be multiplied.\u001b[39m\n",
      "\n",
      "\u001b[33m        Note that these weights will be multiplied with sample_weight (passed\u001b[39m\n",
      "\u001b[33m        through the fit method) if sample_weight is specified.\u001b[39m\n",
      "\n",
      "\u001b[33m    ccp_alpha : non-negative float, default=0.0\u001b[39m\n",
      "\u001b[33m        Complexity parameter used for Minimal Cost-Complexity Pruning. The\u001b[39m\n",
      "\u001b[33m        subtree with the largest cost complexity that is smaller than\u001b[39m\n",
      "\u001b[33m        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\u001b[39m\n",
      "\u001b[33m        :ref:`minimal_cost_complexity_pruning` for details. See\u001b[39m\n",
      "\u001b[33m        :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\u001b[39m\n",
      "\u001b[33m        for an example of such pruning.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.22\u001b[39m\n",
      "\n",
      "\u001b[33m    max_samples : int or float, default=None\u001b[39m\n",
      "\u001b[33m        If bootstrap is True, the number of samples to draw from X\u001b[39m\n",
      "\u001b[33m        to train each base estimator.\u001b[39m\n",
      "\n",
      "\u001b[33m        - If None (default), then draw `X.shape[0]` samples.\u001b[39m\n",
      "\u001b[33m        - If int, then draw `max_samples` samples.\u001b[39m\n",
      "\u001b[33m        - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\u001b[39m\n",
      "\u001b[33m          `max_samples` should be in the interval `(0.0, 1.0]`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.22\u001b[39m\n",
      "\n",
      "\u001b[33m    monotonic_cst : array-like of int of shape (n_features), default=None\u001b[39m\n",
      "\u001b[33m        Indicates the monotonicity constraint to enforce on each feature.\u001b[39m\n",
      "\u001b[33m          - 1: monotonic increase\u001b[39m\n",
      "\u001b[33m          - 0: no constraint\u001b[39m\n",
      "\u001b[33m          - -1: monotonic decrease\u001b[39m\n",
      "\n",
      "\u001b[33m        If monotonic_cst is None, no constraints are applied.\u001b[39m\n",
      "\n",
      "\u001b[33m        Monotonicity constraints are not supported for:\u001b[39m\n",
      "\u001b[33m          - multiclass classifications (i.e. when `n_classes > 2`),\u001b[39m\n",
      "\u001b[33m          - multioutput classifications (i.e. when `n_outputs_ > 1`),\u001b[39m\n",
      "\u001b[33m          - classifications trained on data with missing values.\u001b[39m\n",
      "\n",
      "\u001b[33m        The constraints hold over the probability of the positive class.\u001b[39m\n",
      "\n",
      "\u001b[33m        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 1.4\u001b[39m\n",
      "\n",
      "\u001b[33m    Attributes\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\u001b[39m\n",
      "\u001b[33m        The child estimator template used to create the collection of fitted\u001b[39m\n",
      "\u001b[33m        sub-estimators.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 1.2\u001b[39m\n",
      "\u001b[33m           `base_estimator_` was renamed to `estimator_`.\u001b[39m\n",
      "\n",
      "\u001b[33m    estimators_ : list of DecisionTreeClassifier\u001b[39m\n",
      "\u001b[33m        The collection of fitted sub-estimators.\u001b[39m\n",
      "\n",
      "\u001b[33m    classes_ : ndarray of shape (n_classes,) or a list of such arrays\u001b[39m\n",
      "\u001b[33m        The classes labels (single output problem), or a list of arrays of\u001b[39m\n",
      "\u001b[33m        class labels (multi-output problem).\u001b[39m\n",
      "\n",
      "\u001b[33m    n_classes_ : int or list\u001b[39m\n",
      "\u001b[33m        The number of classes (single output problem), or a list containing the\u001b[39m\n",
      "\u001b[33m        number of classes for each output (multi-output problem).\u001b[39m\n",
      "\n",
      "\u001b[33m    n_features_in_ : int\u001b[39m\n",
      "\u001b[33m        Number of features seen during :term:`fit`.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 0.24\u001b[39m\n",
      "\n",
      "\u001b[33m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[39m\n",
      "\u001b[33m        Names of features seen during :term:`fit`. Defined only when `X`\u001b[39m\n",
      "\u001b[33m        has feature names that are all strings.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 1.0\u001b[39m\n",
      "\n",
      "\u001b[33m    n_outputs_ : int\u001b[39m\n",
      "\u001b[33m        The number of outputs when ``fit`` is performed.\u001b[39m\n",
      "\n",
      "\u001b[33m    feature_importances_ : ndarray of shape (n_features,)\u001b[39m\n",
      "\u001b[33m        The impurity-based feature importances.\u001b[39m\n",
      "\u001b[33m        The higher, the more important the feature.\u001b[39m\n",
      "\u001b[33m        The importance of a feature is computed as the (normalized)\u001b[39m\n",
      "\u001b[33m        total reduction of the criterion brought by that feature.  It is also\u001b[39m\n",
      "\u001b[33m        known as the Gini importance.\u001b[39m\n",
      "\n",
      "\u001b[33m        Warning: impurity-based feature importances can be misleading for\u001b[39m\n",
      "\u001b[33m        high cardinality features (many unique values). See\u001b[39m\n",
      "\u001b[33m        :func:`sklearn.inspection.permutation_importance` as an alternative.\u001b[39m\n",
      "\n",
      "\u001b[33m    oob_score_ : float\u001b[39m\n",
      "\u001b[33m        Score of the training dataset obtained using an out-of-bag estimate.\u001b[39m\n",
      "\u001b[33m        This attribute exists only when ``oob_score`` is True.\u001b[39m\n",
      "\n",
      "\u001b[33m    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \\\u001b[39m\n",
      "\u001b[33m            (n_samples, n_classes, n_outputs)\u001b[39m\n",
      "\u001b[33m        Decision function computed with out-of-bag estimate on the training\u001b[39m\n",
      "\u001b[33m        set. If n_estimators is small it might be possible that a data point\u001b[39m\n",
      "\u001b[33m        was never left out during the bootstrap. In this case,\u001b[39m\n",
      "\u001b[33m        `oob_decision_function_` might contain NaN. This attribute exists\u001b[39m\n",
      "\u001b[33m        only when ``oob_score`` is True.\u001b[39m\n",
      "\n",
      "\u001b[33m    estimators_samples_ : list of arrays\u001b[39m\n",
      "\u001b[33m        The subset of drawn samples (i.e., the in-bag samples) for each base\u001b[39m\n",
      "\u001b[33m        estimator. Each subset is defined by an array of the indices selected.\u001b[39m\n",
      "\n",
      "\u001b[33m        .. versionadded:: 1.4\u001b[39m\n",
      "\n",
      "\u001b[33m    See Also\u001b[39m\n",
      "\u001b[33m    --------\u001b[39m\n",
      "\u001b[33m    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\u001b[39m\n",
      "\u001b[33m    sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\u001b[39m\n",
      "\u001b[33m        tree classifiers.\u001b[39m\n",
      "\u001b[33m    sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\u001b[39m\n",
      "\u001b[33m        Boosting Classification Tree, very fast for big datasets (n_samples >=\u001b[39m\n",
      "\u001b[33m        10_000).\u001b[39m\n",
      "\n",
      "\u001b[33m    Notes\u001b[39m\n",
      "\u001b[33m    -----\u001b[39m\n",
      "\u001b[33m    The default values for the parameters controlling the size of the trees\u001b[39m\n",
      "\u001b[33m    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\u001b[39m\n",
      "\u001b[33m    unpruned trees which can potentially be very large on some data sets. To\u001b[39m\n",
      "\u001b[33m    reduce memory consumption, the complexity and size of the trees should be\u001b[39m\n",
      "\u001b[33m    controlled by setting those parameter values.\u001b[39m\n",
      "\n",
      "\u001b[33m    The features are always randomly permuted at each split. Therefore,\u001b[39m\n",
      "\u001b[33m    the best found split may vary, even with the same training data,\u001b[39m\n",
      "\u001b[33m    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\u001b[39m\n",
      "\u001b[33m    of the criterion is identical for several splits enumerated during the\u001b[39m\n",
      "\u001b[33m    search of the best split. To obtain a deterministic behaviour during\u001b[39m\n",
      "\u001b[33m    fitting, ``random_state`` has to be fixed.\u001b[39m\n",
      "\n",
      "\u001b[33m    References\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\u001b[39m\n",
      "\n",
      "\u001b[33m    Examples\u001b[39m\n",
      "\u001b[33m    --------\u001b[39m\n",
      "\u001b[33m    >>> from sklearn.ensemble import RandomForestClassifier\u001b[39m\n",
      "\u001b[33m    >>> from sklearn.datasets import make_classification\u001b[39m\n",
      "\u001b[33m    >>> X, y = make_classification(n_samples=1000, n_features=4,\u001b[39m\n",
      "\u001b[33m    ...                            n_informative=2, n_redundant=0,\u001b[39m\n",
      "\u001b[33m    ...                            random_state=0, shuffle=False)\u001b[39m\n",
      "\u001b[33m    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\u001b[39m\n",
      "\u001b[33m    >>> clf.fit(X, y)\u001b[39m\n",
      "\u001b[33m    RandomForestClassifier(...)\u001b[39m\n",
      "\u001b[33m    >>> print(clf.predict([[0, 0, 0, 0]]))\u001b[39m\n",
      "\u001b[33m    [1]\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    _parameter_constraints: dict = {\n",
      "        **ForestClassifier._parameter_constraints,\n",
      "        **DecisionTreeClassifier._parameter_constraints,\n",
      "        \u001b[33m\"class_weight\"\u001b[39m: [\n",
      "            StrOptions({\u001b[33m\"balanced_subsample\"\u001b[39m, \u001b[33m\"balanced\"\u001b[39m}),\n",
      "            dict,\n",
      "            list,\n",
      "            \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        ],\n",
      "    }\n",
      "    _parameter_constraints.pop(\u001b[33m\"splitter\"\u001b[39m)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(\n",
      "        self,\n",
      "        n_estimators=\u001b[32m100\u001b[39m,\n",
      "        *,\n",
      "        criterion=\u001b[33m\"gini\"\u001b[39m,\n",
      "        max_depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        min_samples_split=\u001b[32m2\u001b[39m,\n",
      "        min_samples_leaf=\u001b[32m1\u001b[39m,\n",
      "        min_weight_fraction_leaf=\u001b[32m0.0\u001b[39m,\n",
      "        max_features=\u001b[33m\"sqrt\"\u001b[39m,\n",
      "        max_leaf_nodes=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        min_impurity_decrease=\u001b[32m0.0\u001b[39m,\n",
      "        bootstrap=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "        oob_score=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        verbose=\u001b[32m0\u001b[39m,\n",
      "        warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        class_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        ccp_alpha=\u001b[32m0.0\u001b[39m,\n",
      "        max_samples=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        monotonic_cst=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ):\n",
      "        super().__init__(\n",
      "            estimator=DecisionTreeClassifier(),\n",
      "            n_estimators=n_estimators,\n",
      "            estimator_params=(\n",
      "                \u001b[33m\"criterion\"\u001b[39m,\n",
      "                \u001b[33m\"max_depth\"\u001b[39m,\n",
      "                \u001b[33m\"min_samples_split\"\u001b[39m,\n",
      "                \u001b[33m\"min_samples_leaf\"\u001b[39m,\n",
      "                \u001b[33m\"min_weight_fraction_leaf\"\u001b[39m,\n",
      "                \u001b[33m\"max_features\"\u001b[39m,\n",
      "                \u001b[33m\"max_leaf_nodes\"\u001b[39m,\n",
      "                \u001b[33m\"min_impurity_decrease\"\u001b[39m,\n",
      "                \u001b[33m\"random_state\"\u001b[39m,\n",
      "                \u001b[33m\"ccp_alpha\"\u001b[39m,\n",
      "                \u001b[33m\"monotonic_cst\"\u001b[39m,\n",
      "            ),\n",
      "            bootstrap=bootstrap,\n",
      "            oob_score=oob_score,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            verbose=verbose,\n",
      "            warm_start=warm_start,\n",
      "            class_weight=class_weight,\n",
      "            max_samples=max_samples,\n",
      "        )\n",
      "\n",
      "        self.criterion = criterion\n",
      "        self.max_depth = max_depth\n",
      "        self.min_samples_split = min_samples_split\n",
      "        self.min_samples_leaf = min_samples_leaf\n",
      "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
      "        self.max_features = max_features\n",
      "        self.max_leaf_nodes = max_leaf_nodes\n",
      "        self.min_impurity_decrease = min_impurity_decrease\n",
      "        self.monotonic_cst = monotonic_cst\n",
      "        self.ccp_alpha = ccp_alpha\n",
      "\u001b[31mFile:\u001b[39m           c:\\users\\maxi\\miniconda3\\envs\\data_analysis_env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\n",
      "\u001b[31mType:\u001b[39m           ABCMeta\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "RandomForestClassifier??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844c8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
