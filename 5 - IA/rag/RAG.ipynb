{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 游늾 Jupyter Notebook: Construyendo un Sistema RAG con Python\n",
    "\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "En este notebook, vamos a construir un sistema de **Retrieval-Augmented Generation (RAG)** desde sus fundamentos. El objetivo es entender cada componente del proceso sin depender de librer칤as de alto nivel como LangChain o LlamaIndex. Usaremos Python, una base de datos SQLite para actuar como nuestra base de conocimiento vectorial, y las APIs de **OpenAI, Google Gemini y Cohere** para la generaci칩n de embeddings y las respuestas finales.\n",
    "\n",
    "Este enfoque nos permitir치 ver \"debajo del cap칩\" c칩mo funciona RAG, un conocimiento crucial para cualquiera que quiera especializarse en la creaci칩n de aplicaciones con Modelos de Lenguaje Grandes (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 游븷 쯈u칠 es RAG y por qu칠 es importante?\n",
    "\n",
    "Un LLM, como GPT-4, Gemini o Command R+, tiene un conocimiento inmenso, pero limitado a la fecha en que fue entrenado. Adem치s, no conoce tus datos privados o informaci칩n muy espec칤fica y reciente. Esto genera dos problemas principales:\n",
    "\n",
    "1.  **Desconocimiento de datos espec칤ficos:** No puede responder preguntas sobre tus documentos internos, tu base de c칩digo o eventos ocurridos despu칠s de su fecha de corte de conocimiento.\n",
    "2.  **Alucinaciones:** Cuando no sabe una respuesta, a veces puede \"inventar\" informaci칩n que suena plausible pero es incorrecta.\n",
    "\n",
    "**RAG soluciona esto** conectando el LLM a una base de conocimiento externa. El proceso, en pocas palabras, es:\n",
    "\n",
    "1.  **Retrieval (Recuperaci칩n):** Ante una pregunta del usuario, el sistema primero busca informaci칩n relevante en una base de datos (nuestra fuente de conocimiento).\n",
    "2.  **Augmentation (Aumentaci칩n):** La informaci칩n recuperada se a침ade al prompt original del usuario como \"contexto\".\n",
    "3.  **Generation (Generaci칩n):** Se env칤a este prompt \"aumentado\" al LLM, que ahora tiene la informaci칩n necesaria para generar una respuesta precisa y fundamentada en los datos proporcionados.\n",
    "\n",
    "![Diagrama RAG](https://datos.gob.es/sites/default/files/datosgobes/generacion-aumentada_1.jpg)\n",
    "\n",
    "En nuestro caso, la \"base de conocimiento\" ser치 una colecci칩n de funciones de Python, y construiremos un sistema que pueda responder preguntas sobre c칩mo funcionan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 0: Instalaci칩n y Configuraci칩n de Dependencias**\n",
    "\n",
    "Primero, necesitamos instalar las librer칤as necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta y ejecuta la siguiente l칤nea si no tienes instaladas las librer칤as\n",
    "# !pip install openai google-generativeai cohere numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci칩n, importaremos las librer칤as y, lo m치s importante, configuraremos las claves de API.\n",
    "\n",
    "**丘멆잺 춰Importante!** Nunca escribas tus claves de API directamente en el c칩digo. Usa variables de entorno o un gestor de secretos. Por simplicidad en esta clase, las cargaremos desde variables que debes definir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import cohere\n",
    "\n",
    "# --- CONFIGURACI칍N DE LAS API KEYS ---\n",
    "# Aseg칰rate de tener tus claves de API como variables de entorno\n",
    "# o reemplaza \"YOUR_API_KEY\" con tu clave correspondiente.\n",
    "\n",
    "# OpenAI\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # Reemplaza con tu clave de OpenAI\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Google Gemini\n",
    "#os.environ[\"GEMINI_API_KEY\"] = \"AIza...\" # Reemplaza con tu clave de Google\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Cohere\n",
    "#os.environ[\"COHERE_API_KEY\"] = \"...\" # Reemplaza con tu clave de Cohere\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "print(\"Librer칤as importadas y APIs configuradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1: Nuestra Base de Conocimiento (El C칩digo)**\n",
    "\n",
    "Para este ejemplo, nuestra base de conocimiento ser치 una colecci칩n de funciones de Python. En un caso real, esto podr칤a ser una gran base de c칩digo, documentaci칩n o cualquier conjunto de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestra base de conocimiento: una lista de funciones de Python como strings.\n",
    "# Cada funci칩n ser치 un \"documento\" individual en nuestra base de datos.\n",
    "code_snippets = [\n",
    "    \"\"\"\n",
    "def calcular_fibonacci(n):\n",
    "    \\\"\\\"\\\"Calcula el n-칠simo n칰mero de la secuencia de Fibonacci de forma recursiva.\\\"\\\"\\\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return calcular_fibonacci(n-1) + calcular_fibonacci(n-2)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "def es_palindromo(cadena):\n",
    "    \\\"\\\"\\\"Verifica si una cadena de texto es un pal칤ndromo.\\\"\\\"\\\"\n",
    "    cadena = cadena.lower().replace(' ', '')\n",
    "    return cadena == cadena[::-1]\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "def quicksort(arr):\n",
    "    \\\"\\\"\\\"Implementa el algoritmo de ordenamiento Quicksort.\\\"\\\"\\\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quicksort(left) + middle + quicksort(right)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "def buscar_en_lista_ordenada(lista, elemento):\n",
    "    \\\"\\\"\\\"Realiza una b칰squeda binaria en una lista ordenada.\\\"\\\"\\\"\n",
    "    inicio, fin = 0, len(lista) - 1\n",
    "    while inicio <= fin:\n",
    "        medio = (inicio + fin) // 2\n",
    "        if lista[medio] == elemento:\n",
    "            return medio\n",
    "        elif lista[medio] < elemento:\n",
    "            inicio = medio + 1\n",
    "        else:\n",
    "            fin = medio - 1\n",
    "    return -1 # Elemento no encontrado\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(f\"Tenemos {len(code_snippets)} fragmentos de c칩digo en nuestra base de conocimiento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2: Chunking (Fragmentaci칩n)**\n",
    "\n",
    "El \"chunking\" es el proceso de dividir grandes textos en fragmentos m치s peque침os o \"chunks\". 쯇or qu칠 hacemos esto?\n",
    "\n",
    "1.  **L칤mite del Contexto:** Los modelos de embedding tienen un l칤mite en la cantidad de texto que pueden procesar a la vez.\n",
    "2.  **Relevancia:** Fragmentos m치s peque침os y enfocados suelen ser m치s relevantes para una consulta espec칤fica que un documento entero, lo que mejora la calidad de la b칰squeda.\n",
    "\n",
    "En nuestro caso, hemos hecho un \"chunking manual\": cada funci칩n es un chunk. Esto es una estrategia v치lida cuando los documentos son naturalmente modulares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3: Generaci칩n de Embeddings**\n",
    "\n",
    "Un **embedding** es una representaci칩n vectorial (una lista de n칰meros) de un fragmento de texto. La magia de los embeddings es que capturan el *significado sem치ntico* del texto. Textos con significados similares tendr치n vectores que \"apuntan\" en direcciones parecidas en un espacio multidimensional.\n",
    "\n",
    "Crearemos funciones para generar embeddings usando las tres APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para mapear nombres de modelos a sus dimensiones\n",
    "# Esto es 칰til para crear la base de datos correctamente.\n",
    "EMBEDDING_MODELS = {\n",
    "    \"openai\": {\"model\": \"text-embedding-3-small\", \"dimensions\": 1536},\n",
    "    \"gemini\": {\"model\": \"models/text-embedding-004\", \"dimensions\": 768},\n",
    "    \"cohere\": {\"model\": \"embed-multilingual-v3.0\", \"dimensions\": 1024}\n",
    "}\n",
    "\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"Genera embeddings usando la API de OpenAI.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = openai.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def get_gemini_embedding(text, model=\"models/text-embedding-004\"):\n",
    "    \"\"\"Genera embeddings usando la API de Google Gemini.\"\"\"\n",
    "    return genai.embed_content(model=model, content=text)[\"embedding\"]\n",
    "\n",
    "def get_cohere_embedding(text, model=\"embed-multilingual-v3.0\"):\n",
    "    \"\"\"Genera embeddings usando la API de Cohere.\"\"\"\n",
    "    response = co.embed(texts=[text], model=model)\n",
    "    return response.embeddings[0]\n",
    "\n",
    "# Probemos una de las funciones\n",
    "ejemplo_embedding = get_gemini_embedding(\"Hola mundo\")\n",
    "print(\"Ejemplo de embedding con Gemini:\")\n",
    "print(f\"Tipo: {type(ejemplo_embedding)}\")\n",
    "print(f\"Longitud (dimensiones): {len(ejemplo_embedding)}\")\n",
    "print(f\"Primeros 5 valores: {ejemplo_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 4: Almacenamiento en SQLite**\n",
    "\n",
    "Ahora, necesitamos un lugar para guardar nuestros chunks de c칩digo y sus correspondientes embeddings. Usaremos **SQLite**, una base de datos ligera y sin servidor que guarda todo en un 칰nico fichero. Es perfecta para proyectos peque침os y educativos.\n",
    "\n",
    "Crearemos una tabla para almacenar:\n",
    "* `id`: Identificador 칰nico.\n",
    "* `model_name`: El modelo que gener칩 el embedding (ej. 'openai').\n",
    "* `chunk`: El fragmento de c칩digo.\n",
    "* `embedding`: El vector num칠rico. Como SQLite no tiene un tipo de dato \"array\", lo guardaremos como un `BLOB` (Binary Large Object) tras serializarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"rag_database.db\"\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Crea la base de datos y la tabla si no existen.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS code_embeddings (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            model_name TEXT NOT NULL,\n",
    "            chunk TEXT NOT NULL,\n",
    "            embedding BLOB NOT NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    # Limpiamos la tabla para ejecuciones frescas en este notebook\n",
    "    cursor.execute(\"DELETE FROM code_embeddings\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Base de datos y tabla 'code_embeddings' creadas y limpiadas.\")\n",
    "\n",
    "def store_embeddings(model_name, chunks, embeddings):\n",
    "    \"\"\"Almacena los chunks y sus embeddings en la base de datos.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        # Serializamos el embedding (array de numpy) a bytes para guardarlo como BLOB\n",
    "        embedding_blob = np.array(embedding, dtype=np.float32).tobytes()\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO code_embeddings (model_name, chunk, embedding) VALUES (?, ?, ?)\",\n",
    "            (model_name, chunk, embedding_blob)\n",
    "        )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# --- Proceso Principal de Poblaci칩n de la Base de Datos ---\n",
    "\n",
    "setup_database()\n",
    "\n",
    "# Elegimos qu칠 modelo usar para poblar la base de datos.\n",
    "# Podr칤amos hacerlo para los tres, pero para la demo, usemos uno.\n",
    "chosen_model_name = \"gemini\" # Puedes cambiar a \"openai\" o \"cohere\"\n",
    "\n",
    "print(f\"\\nGenerando y almacenando embeddings usando el modelo: {chosen_model_name}...\")\n",
    "\n",
    "embedding_function = None\n",
    "if chosen_model_name == \"openai\":\n",
    "    embedding_function = get_openai_embedding\n",
    "elif chosen_model_name == \"gemini\":\n",
    "    embedding_function = get_gemini_embedding\n",
    "elif chosen_model_name == \"cohere\":\n",
    "    embedding_function = get_cohere_embedding\n",
    "\n",
    "# Generamos los embeddings para cada fragmento de c칩digo\n",
    "embeddings_to_store = [embedding_function(chunk) for chunk in code_snippets]\n",
    "\n",
    "# Los almacenamos en la base de datos\n",
    "store_embeddings(chosen_model_name, code_snippets, embeddings_to_store)\n",
    "\n",
    "print(f\"춰칄xito! Se han almacenado {len(code_snippets)} chunks y sus embeddings en '{DB_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 5: Retrieval (Recuperaci칩n de Informaci칩n)**\n",
    "\n",
    "Esta es la parte \"R\" de RAG. El proceso es:\n",
    "1.  Tomar la pregunta del usuario (la \"query\").\n",
    "2.  Generar un embedding para esa query **usando el mismo modelo que usamos para crear los embeddings de la base de datos**. 춰Esto es crucial!\n",
    "3.  Recuperar todos los embeddings de la base de datos.\n",
    "4.  Calcular la similitud entre el embedding de la query y todos los embeddings almacenados.\n",
    "5.  Seleccionar los `k` fragmentos de c칩digo (chunks) cuyos embeddings son m치s similares.\n",
    "\n",
    "La m칠trica de similitud m치s com칰n es la **similitud del coseno**, que mide el coseno del 치ngulo entre dos vectores. Un valor de 1 significa que son id칠nticos en orientaci칩n, 0 que son ortogonales, y -1 que son opuestos.\n",
    "\n",
    "La f칩rmula es:\n",
    "$$\\text{similitud}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_top_k(query, model_name, top_k=2):\n",
    "    \"\"\"Recupera los k chunks m치s relevantes de la base de datos.\"\"\"\n",
    "    print(f\"Buscando los {top_k} chunks m치s relevantes para la query usando el modelo '{model_name}'...\")\n",
    "\n",
    "    # 1. Generar embedding para la query\n",
    "    embedding_function = None\n",
    "    if model_name == \"openai\":\n",
    "        embedding_function = get_openai_embedding\n",
    "    elif model_name == \"gemini\":\n",
    "        embedding_function = get_gemini_embedding\n",
    "    elif model_name == \"cohere\":\n",
    "        embedding_function = get_cohere_embedding\n",
    "\n",
    "    query_embedding = embedding_function(query)\n",
    "    query_embedding_np = np.array(query_embedding).reshape(1, -1) # Convertir a array 2D\n",
    "\n",
    "    # 2. Obtener todos los chunks y embeddings de la BD\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT chunk, embedding FROM code_embeddings WHERE model_name = ?\", (model_name,))\n",
    "    db_results = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not db_results:\n",
    "        print(\"No se encontraron embeddings en la base de datos para el modelo especificado.\")\n",
    "        return []\n",
    "\n",
    "    db_chunks = [row[0] for row in db_results]\n",
    "    \n",
    "    # Deserializar los embeddings de BLOB a numpy array\n",
    "    embedding_dim = EMBEDDING_MODELS[model_name][\"dimensions\"]\n",
    "    db_embeddings = [np.frombuffer(row[1], dtype=np.float32) for row in db_results]\n",
    "    db_embeddings_np = np.array(db_embeddings)\n",
    "\n",
    "    # 3. Calcular la similitud del coseno\n",
    "    similarities = cosine_similarity(query_embedding_np, db_embeddings_np)[0]\n",
    "\n",
    "    # 4. Encontrar los top-k\n",
    "    # Obtenemos los 칤ndices de las mayores similitudes en orden descendente\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    # 5. Devolver los chunks correspondientes\n",
    "    relevant_chunks = [db_chunks[i] for i in top_k_indices]\n",
    "    \n",
    "    print(\"Chunks recuperados con 칠xito.\")\n",
    "    return relevant_chunks\n",
    "\n",
    "# --- Prueba de Recuperaci칩n ---\n",
    "user_query = \"쯖칩mo puedo ordenar una lista de n칰meros?\"\n",
    "# Usamos el mismo modelo con el que poblamos la BD\n",
    "retrieved_chunks = retrieve_top_k(user_query, chosen_model_name, top_k=1)\n",
    "\n",
    "print(\"\\n--- Contexto Recuperado ---\")\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, el sistema recuper칩 correctamente la funci칩n `quicksort` como el contexto m치s relevante para la pregunta sobre \"ordenar una lista\". 춰El retrieval funciona!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 6: Augmentation y Generation (Aumentaci칩n y Generaci칩n)**\n",
    "\n",
    "Ahora que tenemos el contexto relevante, pasamos a la parte \"G\" de RAG.\n",
    "\n",
    "1.  **Aumentaci칩n:** Creamos un nuevo prompt para el LLM. Este prompt incluir치 el contexto recuperado y la pregunta original del usuario. Un buen formato es clave para que el modelo entienda qu칠 hacer.\n",
    "2.  **Generaci칩n:** Enviamos este prompt aumentado a un LLM (puede ser de OpenAI, Gemini o Cohere) para que genere la respuesta final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, context_chunks):\n",
    "    \"\"\"Construye el prompt aumentado para el LLM.\"\"\"\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Eres un asistente experto en Python. Responde a la pregunta del usuario bas치ndote 칰nicamente en el siguiente contexto de c칩digo.\n",
    "Si el contexto no contiene la respuesta, di que no tienes suficiente informaci칩n.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta del usuario:\n",
    "{query}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_llm_response(prompt, provider=\"gemini\"):\n",
    "    \"\"\"Obtiene una respuesta de un LLM dado un prompt.\"\"\"\n",
    "    print(f\"\\nEnviando prompt aumentado a {provider.upper()}...\")\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        elif provider == \"gemini\":\n",
    "            model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text\n",
    "        elif provider == \"cohere\":\n",
    "            response = co.chat(\n",
    "                model='command-r',\n",
    "                message=prompt\n",
    "            )\n",
    "            return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error al contactar la API de {provider}: {e}\"\n",
    "\n",
    "\n",
    "# --- Proceso Final: Poni칠ndolo Todo Junto ---\n",
    "\n",
    "# 1. Definimos la query del usuario\n",
    "final_user_query = \"쮺u치l es un buen algoritmo para ordenar una lista y c칩mo funciona?\"\n",
    "\n",
    "# 2. Recuperamos el contexto (Retrieval)\n",
    "# Usamos el modelo con el que creamos la BD.\n",
    "context_chunks = retrieve_top_k(final_user_query, chosen_model_name, top_k=1)\n",
    "\n",
    "# 3. Construimos el prompt (Augmentation)\n",
    "final_prompt = build_prompt(final_user_query, context_chunks)\n",
    "\n",
    "print(\"\\n--- Prompt Final Enviado al LLM ---\")\n",
    "print(final_prompt)\n",
    "\n",
    "# 4. Generamos la respuesta final (Generation)\n",
    "# Vamos a probar con los tres modelos para comparar\n",
    "\n",
    "# Respuesta con Gemini\n",
    "response_gemini = get_llm_response(final_prompt, provider=\"gemini\")\n",
    "print(\"\\n--- Respuesta de Gemini ---\")\n",
    "print(response_gemini)\n",
    "\n",
    "# Respuesta con OpenAI\n",
    "response_openai = get_llm_response(final_prompt, provider=\"openai\")\n",
    "print(\"\\n--- Respuesta de OpenAI ---\")\n",
    "print(response_openai)\n",
    "\n",
    "# Respuesta con Cohere\n",
    "response_cohere = get_llm_response(final_prompt, provider=\"cohere\")\n",
    "print(\"\\n--- Respuesta de Cohere ---\")\n",
    "print(response_cohere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones y Pr칩ximos Pasos\n",
    "\n",
    "춰Felicidades! Has construido un sistema RAG completo desde cero.\n",
    "\n",
    "**Hemos aprendido a:**\n",
    "1.  **Fragmentar (Chunking):** Dividir el conocimiento en piezas manejables.\n",
    "2.  **Generar Embeddings:** Convertir texto en vectores sem치nticos usando APIs de vanguardia.\n",
    "3.  **Almacenar:** Guardar estos datos en una base de datos simple como SQLite.\n",
    "4.  **Recuperar (Retrieval):** Implementar una b칰squeda por similitud de coseno para encontrar los chunks m치s relevantes para una query.\n",
    "5.  **Aumentar y Generar (Augmented Generation):** Construir un prompt con contexto y usar un LLM para obtener una respuesta fundamentada.\n",
    "\n",
    "**Para explorar m치s all치:**\n",
    "* **Vector Databases:** En lugar de SQLite y c치lculo manual, investiga bases de datos vectoriales dedicadas como **ChromaDB, FAISS o Pinecone**, que est치n optimizadas para b칰squedas de similitud a gran escala.\n",
    "* **Estrategias de Chunking Avanzadas:** Investiga el chunking sem치ntico o el chunking basado en tokens con solapamiento (`overlapping`).\n",
    "* **Re-ranking:** A veces, los `k` mejores resultados no son los m치s relevantes. Un paso de \"re-ranking\" con un modelo m치s sofisticado (como Cohere Rerank) puede mejorar a칰n m치s la selecci칩n de contexto.\n",
    "* **Interfaz Gr치fica:** 춰Convierte este notebook en una peque침a aplicaci칩n web usando **Streamlit** o **Gradio**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
