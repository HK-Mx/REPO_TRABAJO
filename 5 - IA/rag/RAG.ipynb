{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📔 Jupyter Notebook: Construyendo un Sistema RAG con Python\n",
    "\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "En este notebook, vamos a construir un sistema de **Retrieval-Augmented Generation (RAG)** desde sus fundamentos. El objetivo es entender cada componente del proceso sin depender de librerías de alto nivel como LangChain o LlamaIndex. Usaremos Python, una base de datos SQLite para actuar como nuestra base de conocimiento vectorial, y las APIs de **OpenAI, Google Gemini y Cohere** para la generación de embeddings y las respuestas finales.\n",
    "\n",
    "Este enfoque nos permitirá ver \"debajo del capó\" cómo funciona RAG, un conocimiento crucial para cualquiera que quiera especializarse en la creación de aplicaciones con Modelos de Lenguaje Grandes (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧐 ¿Qué es RAG y por qué es importante?\n",
    "\n",
    "Un LLM, como GPT-4, Gemini o Command R+, tiene un conocimiento inmenso, pero limitado a la fecha en que fue entrenado. Además, no conoce tus datos privados o información muy específica y reciente. Esto genera dos problemas principales:\n",
    "\n",
    "1.  **Desconocimiento de datos específicos:** No puede responder preguntas sobre tus documentos internos, tu base de código o eventos ocurridos después de su fecha de corte de conocimiento.\n",
    "2.  **Alucinaciones:** Cuando no sabe una respuesta, a veces puede \"inventar\" información que suena plausible pero es incorrecta.\n",
    "\n",
    "**RAG soluciona esto** conectando el LLM a una base de conocimiento externa. El proceso, en pocas palabras, es:\n",
    "\n",
    "1.  **Retrieval (Recuperación):** Ante una pregunta del usuario, el sistema primero busca información relevante en una base de datos (nuestra fuente de conocimiento).\n",
    "2.  **Augmentation (Aumentación):** La información recuperada se añade al prompt original del usuario como \"contexto\".\n",
    "3.  **Generation (Generación):** Se envía este prompt \"aumentado\" al LLM, que ahora tiene la información necesaria para generar una respuesta precisa y fundamentada en los datos proporcionados.\n",
    "\n",
    "![Diagrama RAG](https://datos.gob.es/sites/default/files/datosgobes/generacion-aumentada_1.jpg)\n",
    "\n",
    "En nuestro caso, la \"base de conocimiento\" será una colección de funciones de Python, y construiremos un sistema que pueda responder preguntas sobre cómo funcionan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 0: Instalación y Configuración de Dependencias**\n",
    "\n",
    "Primero, necesitamos instalar las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta y ejecuta la siguiente línea si no tienes instaladas las librerías\n",
    "# !pip install openai google-generativeai cohere numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, importaremos las librerías y, lo más importante, configuraremos las claves de API.\n",
    "\n",
    "**⚠️ ¡Importante!** Nunca escribas tus claves de API directamente en el código. Usa variables de entorno o un gestor de secretos. Por simplicidad en esta clase, las cargaremos desde variables que debes definir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import cohere\n",
    "\n",
    "# --- CONFIGURACIÓN DE LAS API KEYS ---\n",
    "# Asegúrate de tener tus claves de API como variables de entorno\n",
    "# o reemplaza \"YOUR_API_KEY\" con tu clave correspondiente.\n",
    "\n",
    "# OpenAI\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # Reemplaza con tu clave de OpenAI\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Google Gemini\n",
    "#os.environ[\"GEMINI_API_KEY\"] = \"AIza...\" # Reemplaza con tu clave de Google\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Cohere\n",
    "#os.environ[\"COHERE_API_KEY\"] = \"...\" # Reemplaza con tu clave de Cohere\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "print(\"Librerías importadas y APIs configuradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1: Nuestra Base de Conocimiento (El Código)**\n",
    "\n",
    "Para este ejemplo, nuestra base de conocimiento será una colección de funciones de Python. En un caso real, esto podría ser una gran base de código, documentación o cualquier conjunto de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestra base de conocimiento: una lista de funciones de Python como strings.\n",
    "# Cada función será un \"documento\" individual en nuestra base de datos.\n",
    "code_snippets = [\n",
    "    \"\"\"\n",
    "def calcular_fibonacci(n):\n",
    "    \\\"\\\"\\\"Calcula el n-ésimo número de la secuencia de Fibonacci de forma recursiva.\\\"\\\"\\\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return calcular_fibonacci(n-1) + calcular_fibonacci(n-2)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "def es_palindromo(cadena):\n",
    "    \\\"\\\"\\\"Verifica si una cadena de texto es un palíndromo.\\\"\\\"\\\"\n",
    "    cadena = cadena.lower().replace(' ', '')\n",
    "    return cadena == cadena[::-1]\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "def quicksort(arr):\n",
    "    \\\"\\\"\\\"Implementa el algoritmo de ordenamiento Quicksort.\\\"\\\"\\\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quicksort(left) + middle + quicksort(right)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "def buscar_en_lista_ordenada(lista, elemento):\n",
    "    \\\"\\\"\\\"Realiza una búsqueda binaria en una lista ordenada.\\\"\\\"\\\"\n",
    "    inicio, fin = 0, len(lista) - 1\n",
    "    while inicio <= fin:\n",
    "        medio = (inicio + fin) // 2\n",
    "        if lista[medio] == elemento:\n",
    "            return medio\n",
    "        elif lista[medio] < elemento:\n",
    "            inicio = medio + 1\n",
    "        else:\n",
    "            fin = medio - 1\n",
    "    return -1 # Elemento no encontrado\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(f\"Tenemos {len(code_snippets)} fragmentos de código en nuestra base de conocimiento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2: Chunking (Fragmentación)**\n",
    "\n",
    "El \"chunking\" es el proceso de dividir grandes textos en fragmentos más pequeños o \"chunks\". ¿Por qué hacemos esto?\n",
    "\n",
    "1.  **Límite del Contexto:** Los modelos de embedding tienen un límite en la cantidad de texto que pueden procesar a la vez.\n",
    "2.  **Relevancia:** Fragmentos más pequeños y enfocados suelen ser más relevantes para una consulta específica que un documento entero, lo que mejora la calidad de la búsqueda.\n",
    "\n",
    "En nuestro caso, hemos hecho un \"chunking manual\": cada función es un chunk. Esto es una estrategia válida cuando los documentos son naturalmente modulares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3: Generación de Embeddings**\n",
    "\n",
    "Un **embedding** es una representación vectorial (una lista de números) de un fragmento de texto. La magia de los embeddings es que capturan el *significado semántico* del texto. Textos con significados similares tendrán vectores que \"apuntan\" en direcciones parecidas en un espacio multidimensional.\n",
    "\n",
    "Crearemos funciones para generar embeddings usando las tres APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para mapear nombres de modelos a sus dimensiones\n",
    "# Esto es útil para crear la base de datos correctamente.\n",
    "EMBEDDING_MODELS = {\n",
    "    \"openai\": {\"model\": \"text-embedding-3-small\", \"dimensions\": 1536},\n",
    "    \"gemini\": {\"model\": \"models/text-embedding-004\", \"dimensions\": 768},\n",
    "    \"cohere\": {\"model\": \"embed-multilingual-v3.0\", \"dimensions\": 1024}\n",
    "}\n",
    "\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"Genera embeddings usando la API de OpenAI.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = openai.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def get_gemini_embedding(text, model=\"models/text-embedding-004\"):\n",
    "    \"\"\"Genera embeddings usando la API de Google Gemini.\"\"\"\n",
    "    return genai.embed_content(model=model, content=text)[\"embedding\"]\n",
    "\n",
    "def get_cohere_embedding(text, model=\"embed-multilingual-v3.0\"):\n",
    "    \"\"\"Genera embeddings usando la API de Cohere.\"\"\"\n",
    "    response = co.embed(texts=[text], model=model)\n",
    "    return response.embeddings[0]\n",
    "\n",
    "# Probemos una de las funciones\n",
    "ejemplo_embedding = get_gemini_embedding(\"Hola mundo\")\n",
    "print(\"Ejemplo de embedding con Gemini:\")\n",
    "print(f\"Tipo: {type(ejemplo_embedding)}\")\n",
    "print(f\"Longitud (dimensiones): {len(ejemplo_embedding)}\")\n",
    "print(f\"Primeros 5 valores: {ejemplo_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 4: Almacenamiento en SQLite**\n",
    "\n",
    "Ahora, necesitamos un lugar para guardar nuestros chunks de código y sus correspondientes embeddings. Usaremos **SQLite**, una base de datos ligera y sin servidor que guarda todo en un único fichero. Es perfecta para proyectos pequeños y educativos.\n",
    "\n",
    "Crearemos una tabla para almacenar:\n",
    "* `id`: Identificador único.\n",
    "* `model_name`: El modelo que generó el embedding (ej. 'openai').\n",
    "* `chunk`: El fragmento de código.\n",
    "* `embedding`: El vector numérico. Como SQLite no tiene un tipo de dato \"array\", lo guardaremos como un `BLOB` (Binary Large Object) tras serializarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"rag_database.db\"\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Crea la base de datos y la tabla si no existen.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS code_embeddings (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            model_name TEXT NOT NULL,\n",
    "            chunk TEXT NOT NULL,\n",
    "            embedding BLOB NOT NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    # Limpiamos la tabla para ejecuciones frescas en este notebook\n",
    "    cursor.execute(\"DELETE FROM code_embeddings\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Base de datos y tabla 'code_embeddings' creadas y limpiadas.\")\n",
    "\n",
    "def store_embeddings(model_name, chunks, embeddings):\n",
    "    \"\"\"Almacena los chunks y sus embeddings en la base de datos.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        # Serializamos el embedding (array de numpy) a bytes para guardarlo como BLOB\n",
    "        embedding_blob = np.array(embedding, dtype=np.float32).tobytes()\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO code_embeddings (model_name, chunk, embedding) VALUES (?, ?, ?)\",\n",
    "            (model_name, chunk, embedding_blob)\n",
    "        )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# --- Proceso Principal de Población de la Base de Datos ---\n",
    "\n",
    "setup_database()\n",
    "\n",
    "# Elegimos qué modelo usar para poblar la base de datos.\n",
    "# Podríamos hacerlo para los tres, pero para la demo, usemos uno.\n",
    "chosen_model_name = \"gemini\" # Puedes cambiar a \"openai\" o \"cohere\"\n",
    "\n",
    "print(f\"\\nGenerando y almacenando embeddings usando el modelo: {chosen_model_name}...\")\n",
    "\n",
    "embedding_function = None\n",
    "if chosen_model_name == \"openai\":\n",
    "    embedding_function = get_openai_embedding\n",
    "elif chosen_model_name == \"gemini\":\n",
    "    embedding_function = get_gemini_embedding\n",
    "elif chosen_model_name == \"cohere\":\n",
    "    embedding_function = get_cohere_embedding\n",
    "\n",
    "# Generamos los embeddings para cada fragmento de código\n",
    "embeddings_to_store = [embedding_function(chunk) for chunk in code_snippets]\n",
    "\n",
    "# Los almacenamos en la base de datos\n",
    "store_embeddings(chosen_model_name, code_snippets, embeddings_to_store)\n",
    "\n",
    "print(f\"¡Éxito! Se han almacenado {len(code_snippets)} chunks y sus embeddings en '{DB_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 5: Retrieval (Recuperación de Información)**\n",
    "\n",
    "Esta es la parte \"R\" de RAG. El proceso es:\n",
    "1.  Tomar la pregunta del usuario (la \"query\").\n",
    "2.  Generar un embedding para esa query **usando el mismo modelo que usamos para crear los embeddings de la base de datos**. ¡Esto es crucial!\n",
    "3.  Recuperar todos los embeddings de la base de datos.\n",
    "4.  Calcular la similitud entre el embedding de la query y todos los embeddings almacenados.\n",
    "5.  Seleccionar los `k` fragmentos de código (chunks) cuyos embeddings son más similares.\n",
    "\n",
    "La métrica de similitud más común es la **similitud del coseno**, que mide el coseno del ángulo entre dos vectores. Un valor de 1 significa que son idénticos en orientación, 0 que son ortogonales, y -1 que son opuestos.\n",
    "\n",
    "La fórmula es:\n",
    "$$\\text{similitud}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_top_k(query, model_name, top_k=2):\n",
    "    \"\"\"Recupera los k chunks más relevantes de la base de datos.\"\"\"\n",
    "    print(f\"Buscando los {top_k} chunks más relevantes para la query usando el modelo '{model_name}'...\")\n",
    "\n",
    "    # 1. Generar embedding para la query\n",
    "    embedding_function = None\n",
    "    if model_name == \"openai\":\n",
    "        embedding_function = get_openai_embedding\n",
    "    elif model_name == \"gemini\":\n",
    "        embedding_function = get_gemini_embedding\n",
    "    elif model_name == \"cohere\":\n",
    "        embedding_function = get_cohere_embedding\n",
    "\n",
    "    query_embedding = embedding_function(query)\n",
    "    query_embedding_np = np.array(query_embedding).reshape(1, -1) # Convertir a array 2D\n",
    "\n",
    "    # 2. Obtener todos los chunks y embeddings de la BD\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT chunk, embedding FROM code_embeddings WHERE model_name = ?\", (model_name,))\n",
    "    db_results = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not db_results:\n",
    "        print(\"No se encontraron embeddings en la base de datos para el modelo especificado.\")\n",
    "        return []\n",
    "\n",
    "    db_chunks = [row[0] for row in db_results]\n",
    "    \n",
    "    # Deserializar los embeddings de BLOB a numpy array\n",
    "    embedding_dim = EMBEDDING_MODELS[model_name][\"dimensions\"]\n",
    "    db_embeddings = [np.frombuffer(row[1], dtype=np.float32) for row in db_results]\n",
    "    db_embeddings_np = np.array(db_embeddings)\n",
    "\n",
    "    # 3. Calcular la similitud del coseno\n",
    "    similarities = cosine_similarity(query_embedding_np, db_embeddings_np)[0]\n",
    "\n",
    "    # 4. Encontrar los top-k\n",
    "    # Obtenemos los índices de las mayores similitudes en orden descendente\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    # 5. Devolver los chunks correspondientes\n",
    "    relevant_chunks = [db_chunks[i] for i in top_k_indices]\n",
    "    \n",
    "    print(\"Chunks recuperados con éxito.\")\n",
    "    return relevant_chunks\n",
    "\n",
    "# --- Prueba de Recuperación ---\n",
    "user_query = \"¿cómo puedo ordenar una lista de números?\"\n",
    "# Usamos el mismo modelo con el que poblamos la BD\n",
    "retrieved_chunks = retrieve_top_k(user_query, chosen_model_name, top_k=1)\n",
    "\n",
    "print(\"\\n--- Contexto Recuperado ---\")\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, el sistema recuperó correctamente la función `quicksort` como el contexto más relevante para la pregunta sobre \"ordenar una lista\". ¡El retrieval funciona!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 6: Augmentation y Generation (Aumentación y Generación)**\n",
    "\n",
    "Ahora que tenemos el contexto relevante, pasamos a la parte \"G\" de RAG.\n",
    "\n",
    "1.  **Aumentación:** Creamos un nuevo prompt para el LLM. Este prompt incluirá el contexto recuperado y la pregunta original del usuario. Un buen formato es clave para que el modelo entienda qué hacer.\n",
    "2.  **Generación:** Enviamos este prompt aumentado a un LLM (puede ser de OpenAI, Gemini o Cohere) para que genere la respuesta final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, context_chunks):\n",
    "    \"\"\"Construye el prompt aumentado para el LLM.\"\"\"\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Eres un asistente experto en Python. Responde a la pregunta del usuario basándote únicamente en el siguiente contexto de código.\n",
    "Si el contexto no contiene la respuesta, di que no tienes suficiente información.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta del usuario:\n",
    "{query}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_llm_response(prompt, provider=\"gemini\"):\n",
    "    \"\"\"Obtiene una respuesta de un LLM dado un prompt.\"\"\"\n",
    "    print(f\"\\nEnviando prompt aumentado a {provider.upper()}...\")\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        elif provider == \"gemini\":\n",
    "            model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text\n",
    "        elif provider == \"cohere\":\n",
    "            response = co.chat(\n",
    "                model='command-r',\n",
    "                message=prompt\n",
    "            )\n",
    "            return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error al contactar la API de {provider}: {e}\"\n",
    "\n",
    "\n",
    "# --- Proceso Final: Poniéndolo Todo Junto ---\n",
    "\n",
    "# 1. Definimos la query del usuario\n",
    "final_user_query = \"¿Cuál es un buen algoritmo para ordenar una lista y cómo funciona?\"\n",
    "\n",
    "# 2. Recuperamos el contexto (Retrieval)\n",
    "# Usamos el modelo con el que creamos la BD.\n",
    "context_chunks = retrieve_top_k(final_user_query, chosen_model_name, top_k=1)\n",
    "\n",
    "# 3. Construimos el prompt (Augmentation)\n",
    "final_prompt = build_prompt(final_user_query, context_chunks)\n",
    "\n",
    "print(\"\\n--- Prompt Final Enviado al LLM ---\")\n",
    "print(final_prompt)\n",
    "\n",
    "# 4. Generamos la respuesta final (Generation)\n",
    "# Vamos a probar con los tres modelos para comparar\n",
    "\n",
    "# Respuesta con Gemini\n",
    "response_gemini = get_llm_response(final_prompt, provider=\"gemini\")\n",
    "print(\"\\n--- Respuesta de Gemini ---\")\n",
    "print(response_gemini)\n",
    "\n",
    "# Respuesta con OpenAI\n",
    "response_openai = get_llm_response(final_prompt, provider=\"openai\")\n",
    "print(\"\\n--- Respuesta de OpenAI ---\")\n",
    "print(response_openai)\n",
    "\n",
    "# Respuesta con Cohere\n",
    "response_cohere = get_llm_response(final_prompt, provider=\"cohere\")\n",
    "print(\"\\n--- Respuesta de Cohere ---\")\n",
    "print(response_cohere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones y Próximos Pasos\n",
    "\n",
    "¡Felicidades! Has construido un sistema RAG completo desde cero.\n",
    "\n",
    "**Hemos aprendido a:**\n",
    "1.  **Fragmentar (Chunking):** Dividir el conocimiento en piezas manejables.\n",
    "2.  **Generar Embeddings:** Convertir texto en vectores semánticos usando APIs de vanguardia.\n",
    "3.  **Almacenar:** Guardar estos datos en una base de datos simple como SQLite.\n",
    "4.  **Recuperar (Retrieval):** Implementar una búsqueda por similitud de coseno para encontrar los chunks más relevantes para una query.\n",
    "5.  **Aumentar y Generar (Augmented Generation):** Construir un prompt con contexto y usar un LLM para obtener una respuesta fundamentada.\n",
    "\n",
    "**Para explorar más allá:**\n",
    "* **Vector Databases:** En lugar de SQLite y cálculo manual, investiga bases de datos vectoriales dedicadas como **ChromaDB, FAISS o Pinecone**, que están optimizadas para búsquedas de similitud a gran escala.\n",
    "* **Estrategias de Chunking Avanzadas:** Investiga el chunking semántico o el chunking basado en tokens con solapamiento (`overlapping`).\n",
    "* **Re-ranking:** A veces, los `k` mejores resultados no son los más relevantes. Un paso de \"re-ranking\" con un modelo más sofisticado (como Cohere Rerank) puede mejorar aún más la selección de contexto.\n",
    "* **Interfaz Gráfica:** ¡Convierte este notebook en una pequeña aplicación web usando **Streamlit** o **Gradio**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
