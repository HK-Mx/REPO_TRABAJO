{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 游늾 Jupyter Notebook: RAG Avanzado con PDFs y Chunking\n",
    "\n",
    "## Objetivo de la Clase\n",
    "\n",
    "En esta versi칩n del notebook, damos un paso m치s all치. En lugar de usar fragmentos de c칩digo pre-definidos, vamos a construir un sistema RAG que consume un **documento PDF** como fuente de conocimiento. Esto nos obligar치 a implementar uno de los pasos m치s cruciales del proceso: el **Chunking (fragmentaci칩n)**.\n",
    "\n",
    "Aprenderemos a:\n",
    "1. Extraer texto de un archivo PDF.\n",
    "2. Dividir un texto largo en fragmentos (chunks) de un tama침o fijo.\n",
    "3. Utilizar estos chunks para construir la base de datos vectorial y responder preguntas sobre el contenido del documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 0: Instalaci칩n y Configuraci칩n de Dependencias**\n",
    "\n",
    "Adem치s de las librer칤as anteriores, ahora necesitamos una para leer archivos PDF. Usaremos `PyMuPDF`, que es r치pida y eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta y ejecuta la siguiente l칤nea si no tienes instaladas las librer칤as\n",
    "# !pip install openai google-generativeai cohere numpy scikit-learn PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import cohere\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# --- CONFIGURACI칍N DE LAS API KEYS ---\n",
    "# Aseg칰rate de tener tus claves de API como variables de entorno\n",
    "# o reemplaza el placeholder con tu clave correspondiente.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"YOUR_GEMINI_API_KEY\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"YOUR_COHERE_API_KEY\"\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "print(\"Librer칤as importadas y APIs configuradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1: Preparar la Base de Conocimiento (El PDF)**\n",
    "\n",
    "Primero, necesitamos nuestro documento. Para que este notebook sea autocontenido, crearemos un PDF de ejemplo con un texto largo sobre la historia de la IA. Luego, implementaremos una funci칩n para extraer todo el texto de ese PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_pdf_de_ejemplo(nombre_archivo, texto):\n",
    "    \"\"\"Crea un archivo PDF simple con el texto proporcionado.\"\"\"\n",
    "    doc = fitz.open() # Nuevo documento PDF\n",
    "    pagina = doc.new_page()\n",
    "    # Inserta el texto. 'rect' define el 치rea donde se insertar치 el texto.\n",
    "    rect = fitz.Rect(50, 50, 550, 800) # x0, y0, x1, y1\n",
    "    pagina.insert_text(texto, rect, fontsize=12, fontname=\"helv\", rotate=0)\n",
    "    doc.save(nombre_archivo)\n",
    "    doc.close()\n",
    "    print(f\"PDF de ejemplo '{nombre_archivo}' creado con 칠xito.\")\n",
    "\n",
    "def extraer_texto_de_pdf(ruta_pdf):\n",
    "    \"\"\"Extrae el texto completo de un archivo PDF.\"\"\"\n",
    "    doc = fitz.open(ruta_pdf)\n",
    "    texto_completo = \"\"\n",
    "    for pagina in doc:\n",
    "        texto_completo += pagina.get_text()\n",
    "    doc.close()\n",
    "    return texto_completo\n",
    "\n",
    "# Contenido para nuestro PDF de ejemplo\n",
    "texto_ia = \"\"\"\n",
    "Historia de la Inteligencia Artificial\n",
    "\n",
    "La historia de la inteligencia artificial (IA) es fascinante y se remonta a la antig칲edad, con mitos e historias sobre seres artificiales dotados de inteligencia. Sin embargo, el campo moderno de la IA no comenz칩 a tomar forma hasta mediados del siglo XX, impulsado por avances en la computaci칩n.\n",
    "\n",
    "La conferencia de Dartmouth en 1956 es ampliamente considerada como el evento que acu침칩 el t칠rmino \"inteligencia artificial\" y lanz칩 el campo como un 치rea formal de investigaci칩n. John McCarthy, Marvin Minsky, Nathaniel Rochester y Claude Shannon organizaron este taller con el objetivo de explorar la conjetura de que cada aspecto del aprendizaje o cualquier otra caracter칤stica de la inteligencia puede, en principio, ser descrito con tanta precisi칩n que se puede hacer que una m치quina lo simule. Este evento marc칩 el inicio de d칠cadas de investigaci칩n, caracterizadas por olas de optimismo y per칤odos de \"invierno de la IA\", donde la financiaci칩n y el inter칠s disminuyeron.\n",
    "\n",
    "Uno de los pioneros m치s importantes fue Alan Turing, un matem치tico y l칩gico brit치nico. En su art칤culo de 1950, \"Computing Machinery and Intelligence\", Turing propuso lo que ahora se conoce como la Prueba de Turing. Esta prueba eval칰a la capacidad de una m치quina para exhibir un comportamiento inteligente indistinguible del de un ser humano. Turing no solo sent칩 las bases te칩ricas de la computaci칩n con su concepto de la M치quina de Turing, sino que tambi칠n plante칩 preguntas filos칩ficas profundas sobre la naturaleza de la mente y la inteligencia que siguen siendo relevantes hoy en d칤a. Su trabajo fue fundamental para el desarrollo de la inform치tica y la IA.\n",
    "\n",
    "Durante los a침os 60 y 70, la investigaci칩n se centr칩 en la resoluci칩n de problemas y los m칠todos simb칩licos. Programas como el \"General Problem Solver\" de Newell y Simon intentaron imitar el pensamiento humano para resolver problemas l칩gicos. Sin embargo, estas primeras aproximaciones tropezaron con la \"explosi칩n combinatoria\": a medida que los problemas se volv칤an m치s complejos, la cantidad de posibles soluciones a explorar crec칤a exponencialmente, haciendo que los c치lculos fueran inviables.\n",
    "\n",
    "El resurgimiento de la IA en los a침os 80 vino de la mano de los sistemas expertos, programas dise침ados para emular la capacidad de toma de decisiones de un experto humano en un dominio espec칤fico. Aunque tuvieron 칠xito comercial, su creaci칩n era costosa y su conocimiento, fr치gil y dif칤cil de mantener.\n",
    "\n",
    "El verdadero cambio de paradigma lleg칩 con el auge del aprendizaje autom치tico (machine learning) en los a침os 90 y 2000, y m치s espec칤ficamente, con el aprendizaje profundo (deep learning) a partir de 2010. En lugar de programar reglas expl칤citas, los modelos de aprendizaje autom치tico aprenden patrones directamente de los datos. Geoffrey Hinton, Yann LeCun y Yoshua Bengio, a menudo llamados los \"padrinos de la IA\", fueron pioneros en el desarrollo de redes neuronales profundas. Sus contribuciones en 치reas como las redes neuronales convolucionales (CNN) para la visi칩n por computadora y las redes neuronales recurrentes (RNN) para el procesamiento del lenguaje natural revolucionaron el campo. El avance en la capacidad de c칩mputo, especialmente con las GPUs, y la disponibilidad de grandes conjuntos de datos (Big Data) fueron los catalizadores que permitieron que el deep learning floreciera.\n",
    "\n",
    "Hoy, la IA est치 en todas partes, desde los asistentes de voz en nuestros tel칠fonos hasta los algoritmos que recomiendan contenido en plataformas de streaming y los modelos de lenguaje grandes como GPT y Gemini, que pueden generar texto coherente y responder preguntas complejas. La investigaci칩n contin칰a a un ritmo acelerado, explorando nuevas arquitecturas, abordando problemas de 칠tica y seguridad, y empujando los l칤mites de lo que las m치quinas pueden aprender y hacer.\n",
    "\"\"\"\n",
    "\n",
    "PDF_FILENAME = \"historia_ia.pdf\"\n",
    "crear_pdf_de_ejemplo(PDF_FILENAME, texto_ia)\n",
    "\n",
    "# Extraer el texto del PDF que acabamos de crear\n",
    "texto_extraido = extraer_texto_de_pdf(PDF_FILENAME)\n",
    "\n",
    "print(\"\\n--- Inicio del Texto Extra칤do del PDF ---\")\n",
    "print(texto_extraido[:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2: Chunking (Fragmentaci칩n) del Texto**\n",
    "\n",
    "Ahora que tenemos el texto completo, debemos dividirlo en chunks. Una estrategia simple y efectiva es el **chunking de tama침o fijo**. Dividiremos el texto en fragmentos de un n칰mero determinado de caracteres.\n",
    "\n",
    "Elegir el tama침o del chunk es importante:\n",
    "- **Chunks demasiado grandes:** Pueden contener informaci칩n irrelevante que diluya el contexto 칰til.\n",
    "- **Chunks demasiado peque침os:** Pueden carecer del contexto necesario para que el embedding capture el significado completo.\n",
    "\n",
    "Para este ejemplo, usaremos un tama침o de **2000 caracteres**, como se solicit칩."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_en_chunks(texto, chunk_size=2000):\n",
    "    \"\"\"Divide un texto en chunks de un tama침o espec칤fico.\"\"\"\n",
    "    return [texto[i:i + chunk_size] for i in range(0, len(texto), chunk_size)]\n",
    "\n",
    "CHUNK_SIZE = 2000\n",
    "text_chunks = dividir_en_chunks(texto_extraido, chunk_size=CHUNK_SIZE)\n",
    "\n",
    "print(f\"El texto ha sido dividido en {len(text_chunks)} chunks.\")\n",
    "print(\"\\n--- Ejemplo del Primer Chunk ---\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3: Generaci칩n de Embeddings**\n",
    "\n",
    "Este paso no cambia. Las funciones que creamos para generar embeddings pueden tomar cualquier cadena de texto, as칤 que funcionar치n perfectamente con nuestros nuevos chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para mapear nombres de modelos a sus dimensiones\n",
    "EMBEDDING_MODELS = {\n",
    "    \"openai\": {\"model\": \"text-embedding-3-small\", \"dimensions\": 1536},\n",
    "    \"gemini\": {\"model\": \"models/text-embedding-004\", \"dimensions\": 768},\n",
    "    \"cohere\": {\"model\": \"embed-multilingual-v3.0\", \"dimensions\": 1024}\n",
    "}\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = openai.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def get_gemini_embedding(text, model=\"models/text-embedding-004\"):\n",
    "    return genai.embed_content(model=model, content=text)[\"embedding\"]\n",
    "\n",
    "def get_cohere_embedding(text, model=\"embed-multilingual-v3.0\"):\n",
    "    response = co.embed(texts=[text], model=model)\n",
    "    return response.embeddings[0]\n",
    "\n",
    "print(\"Funciones de embedding definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 4: Almacenamiento en SQLite**\n",
    "\n",
    "Ahora, poblaremos nuestra base de datos SQLite con los chunks de texto del PDF y sus correspondientes embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"rag_database_pdf.db\"\n",
    "\n",
    "def setup_database():\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS text_embeddings\") # Empezar de cero\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE text_embeddings (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            model_name TEXT NOT NULL,\n",
    "            chunk TEXT NOT NULL,\n",
    "            embedding BLOB NOT NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"Base de datos '{DB_FILE}' y tabla 'text_embeddings' creadas.\")\n",
    "\n",
    "def store_embeddings(model_name, chunks, embeddings):\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        embedding_blob = np.array(embedding, dtype=np.float32).tobytes()\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO text_embeddings (model_name, chunk, embedding) VALUES (?, ?, ?)\",\n",
    "        (model_name, chunk, embedding_blob)\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# --- Proceso Principal de Poblaci칩n de la Base de Datos ---\n",
    "setup_database()\n",
    "\n",
    "chosen_model_name = \"gemini\" # Puedes cambiar a \"openai\" o \"cohere\"\n",
    "print(f\"\\nGenerando y almacenando embeddings para {len(text_chunks)} chunks usando el modelo: {chosen_model_name}...\")\n",
    "\n",
    "embedding_function = get_gemini_embedding # Asignamos la funci칩n directamente\n",
    "\n",
    "# Generamos los embeddings para cada chunk de texto\n",
    "embeddings_to_store = [embedding_function(chunk) for chunk in text_chunks]\n",
    "\n",
    "# Los almacenamos en la base de datos\n",
    "store_embeddings(chosen_model_name, text_chunks, embeddings_to_store)\n",
    "\n",
    "print(f\"춰칄xito! Se han almacenado {len(text_chunks)} chunks y sus embeddings en '{DB_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 5: Retrieval (Recuperaci칩n de Informaci칩n)**\n",
    "\n",
    "La l칩gica de recuperaci칩n es id칠ntica a la del ejemplo anterior. La funci칩n buscar치 en la nueva tabla y comparar치 la query del usuario con los embeddings de los chunks del PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_top_k(query, model_name, top_k=2):\n",
    "    print(f\"Buscando los {top_k} chunks m치s relevantes para la query usando el modelo '{model_name}'...\")\n",
    "\n",
    "    embedding_function = get_gemini_embedding\n",
    "    query_embedding = embedding_function(query)\n",
    "    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n",
    "\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT chunk, embedding FROM text_embeddings WHERE model_name = ?\", (model_name,))\n",
    "    db_results = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not db_results:\n",
    "        print(\"No se encontraron embeddings en la base de datos.\")\n",
    "        return []\n",
    "\n",
    "    db_chunks = [row[0] for row in db_results]\n",
    "    embedding_dim = EMBEDDING_MODELS[model_name][\"dimensions\"]\n",
    "    db_embeddings = [np.frombuffer(row[1], dtype=np.float32) for row in db_results]\n",
    "    db_embeddings_np = np.array(db_embeddings)\n",
    "\n",
    "    similarities = cosine_similarity(query_embedding_np, db_embeddings_np)[0]\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    relevant_chunks = [db_chunks[i] for i in top_k_indices]\n",
    "    \n",
    "    print(\"Chunks recuperados con 칠xito.\")\n",
    "    return relevant_chunks\n",
    "\n",
    "# --- Prueba de Recuperaci칩n ---\n",
    "user_query = \"쯈ui칠n fue Alan Turing y cu치l fue su contribuci칩n a la IA?\"\n",
    "retrieved_chunks = retrieve_top_k(user_query, chosen_model_name, top_k=1)\n",
    "\n",
    "print(\"\\n--- Contexto Recuperado para la pregunta sobre Alan Turing ---\")\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 6: Augmentation y Generation**\n",
    "\n",
    "Finalmente, usamos los chunks recuperados del PDF para aumentar el prompt y generar una respuesta precisa y contextualizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, context_chunks):\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "    prompt = f\"\"\"\n",
    "Eres un asistente experto que responde preguntas sobre un documento. Responde a la pregunta del usuario bas치ndote 칰nicamente en el siguiente contexto extra칤do del documento.\n",
    "Si el contexto no contiene la informaci칩n necesaria para responder, di expl칤citamente: 'La informaci칩n no se encuentra en el documento proporcionado'.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta del usuario:\n",
    "{query}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_llm_response(prompt, provider=\"gemini\"):\n",
    "    print(f\"\\nEnviando prompt aumentado a {provider.upper()}...\")\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        elif provider == \"gemini\":\n",
    "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text\n",
    "        elif provider == \"cohere\":\n",
    "            response = co.chat(model='command-r', message=prompt)\n",
    "            return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error al contactar la API de {provider}: {e}\"\n",
    "\n",
    "# --- Proceso Final: Poni칠ndolo Todo Junto ---\n",
    "final_user_query = \"쯈u칠 fue la conferencia de Dartmouth y por qu칠 es importante para la IA?\"\n",
    "context_chunks = retrieve_top_k(final_user_query, chosen_model_name, top_k=2)\n",
    "final_prompt = build_prompt(final_user_query, context_chunks)\n",
    "\n",
    "print(\"\\n--- Prompt Final Enviado al LLM ---\")\n",
    "print(final_prompt)\n",
    "\n",
    "# --- Generaci칩n con Gemini ---\n",
    "response_gemini = get_llm_response(final_prompt, provider=\"gemini\")\n",
    "print(\"\\n--- Respuesta de Gemini ---\")\n",
    "print(response_gemini)\n",
    "\n",
    "# --- Generaci칩n con OpenAI ---\n",
    "response_openai = get_llm_response(final_prompt, provider=\"openai\")\n",
    "print(\"\\n--- Respuesta de OpenAI ---\")\n",
    "print(response_openai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
