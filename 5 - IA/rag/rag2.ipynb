{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📔 Jupyter Notebook: RAG Avanzado con PDFs y Chunking\n",
    "\n",
    "## Objetivo de la Clase\n",
    "\n",
    "En esta versión del notebook, damos un paso más allá. En lugar de usar fragmentos de código pre-definidos, vamos a construir un sistema RAG que consume un **documento PDF** como fuente de conocimiento. Esto nos obligará a implementar uno de los pasos más cruciales del proceso: el **Chunking (fragmentación)**.\n",
    "\n",
    "Aprenderemos a:\n",
    "1. Extraer texto de un archivo PDF.\n",
    "2. Dividir un texto largo en fragmentos (chunks) de un tamaño fijo.\n",
    "3. Utilizar estos chunks para construir la base de datos vectorial y responder preguntas sobre el contenido del documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 0: Instalación y Configuración de Dependencias**\n",
    "\n",
    "Además de las librerías anteriores, ahora necesitamos una para leer archivos PDF. Usaremos `PyMuPDF`, que es rápida y eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta y ejecuta la siguiente línea si no tienes instaladas las librerías\n",
    "# !pip install openai google-generativeai cohere numpy scikit-learn PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import cohere\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# --- CONFIGURACIÓN DE LAS API KEYS ---\n",
    "# Asegúrate de tener tus claves de API como variables de entorno\n",
    "# o reemplaza el placeholder con tu clave correspondiente.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"YOUR_GEMINI_API_KEY\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"YOUR_COHERE_API_KEY\"\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "print(\"Librerías importadas y APIs configuradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1: Preparar la Base de Conocimiento (El PDF)**\n",
    "\n",
    "Primero, necesitamos nuestro documento. Para que este notebook sea autocontenido, crearemos un PDF de ejemplo con un texto largo sobre la historia de la IA. Luego, implementaremos una función para extraer todo el texto de ese PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_pdf_de_ejemplo(nombre_archivo, texto):\n",
    "    \"\"\"Crea un archivo PDF simple con el texto proporcionado.\"\"\"\n",
    "    doc = fitz.open() # Nuevo documento PDF\n",
    "    pagina = doc.new_page()\n",
    "    # Inserta el texto. 'rect' define el área donde se insertará el texto.\n",
    "    rect = fitz.Rect(50, 50, 550, 800) # x0, y0, x1, y1\n",
    "    pagina.insert_text(texto, rect, fontsize=12, fontname=\"helv\", rotate=0)\n",
    "    doc.save(nombre_archivo)\n",
    "    doc.close()\n",
    "    print(f\"PDF de ejemplo '{nombre_archivo}' creado con éxito.\")\n",
    "\n",
    "def extraer_texto_de_pdf(ruta_pdf):\n",
    "    \"\"\"Extrae el texto completo de un archivo PDF.\"\"\"\n",
    "    doc = fitz.open(ruta_pdf)\n",
    "    texto_completo = \"\"\n",
    "    for pagina in doc:\n",
    "        texto_completo += pagina.get_text()\n",
    "    doc.close()\n",
    "    return texto_completo\n",
    "\n",
    "# Contenido para nuestro PDF de ejemplo\n",
    "texto_ia = \"\"\"\n",
    "Historia de la Inteligencia Artificial\n",
    "\n",
    "La historia de la inteligencia artificial (IA) es fascinante y se remonta a la antigüedad, con mitos e historias sobre seres artificiales dotados de inteligencia. Sin embargo, el campo moderno de la IA no comenzó a tomar forma hasta mediados del siglo XX, impulsado por avances en la computación.\n",
    "\n",
    "La conferencia de Dartmouth en 1956 es ampliamente considerada como el evento que acuñó el término \"inteligencia artificial\" y lanzó el campo como un área formal de investigación. John McCarthy, Marvin Minsky, Nathaniel Rochester y Claude Shannon organizaron este taller con el objetivo de explorar la conjetura de que cada aspecto del aprendizaje o cualquier otra característica de la inteligencia puede, en principio, ser descrito con tanta precisión que se puede hacer que una máquina lo simule. Este evento marcó el inicio de décadas de investigación, caracterizadas por olas de optimismo y períodos de \"invierno de la IA\", donde la financiación y el interés disminuyeron.\n",
    "\n",
    "Uno de los pioneros más importantes fue Alan Turing, un matemático y lógico británico. En su artículo de 1950, \"Computing Machinery and Intelligence\", Turing propuso lo que ahora se conoce como la Prueba de Turing. Esta prueba evalúa la capacidad de una máquina para exhibir un comportamiento inteligente indistinguible del de un ser humano. Turing no solo sentó las bases teóricas de la computación con su concepto de la Máquina de Turing, sino que también planteó preguntas filosóficas profundas sobre la naturaleza de la mente y la inteligencia que siguen siendo relevantes hoy en día. Su trabajo fue fundamental para el desarrollo de la informática y la IA.\n",
    "\n",
    "Durante los años 60 y 70, la investigación se centró en la resolución de problemas y los métodos simbólicos. Programas como el \"General Problem Solver\" de Newell y Simon intentaron imitar el pensamiento humano para resolver problemas lógicos. Sin embargo, estas primeras aproximaciones tropezaron con la \"explosión combinatoria\": a medida que los problemas se volvían más complejos, la cantidad de posibles soluciones a explorar crecía exponencialmente, haciendo que los cálculos fueran inviables.\n",
    "\n",
    "El resurgimiento de la IA en los años 80 vino de la mano de los sistemas expertos, programas diseñados para emular la capacidad de toma de decisiones de un experto humano en un dominio específico. Aunque tuvieron éxito comercial, su creación era costosa y su conocimiento, frágil y difícil de mantener.\n",
    "\n",
    "El verdadero cambio de paradigma llegó con el auge del aprendizaje automático (machine learning) en los años 90 y 2000, y más específicamente, con el aprendizaje profundo (deep learning) a partir de 2010. En lugar de programar reglas explícitas, los modelos de aprendizaje automático aprenden patrones directamente de los datos. Geoffrey Hinton, Yann LeCun y Yoshua Bengio, a menudo llamados los \"padrinos de la IA\", fueron pioneros en el desarrollo de redes neuronales profundas. Sus contribuciones en áreas como las redes neuronales convolucionales (CNN) para la visión por computadora y las redes neuronales recurrentes (RNN) para el procesamiento del lenguaje natural revolucionaron el campo. El avance en la capacidad de cómputo, especialmente con las GPUs, y la disponibilidad de grandes conjuntos de datos (Big Data) fueron los catalizadores que permitieron que el deep learning floreciera.\n",
    "\n",
    "Hoy, la IA está en todas partes, desde los asistentes de voz en nuestros teléfonos hasta los algoritmos que recomiendan contenido en plataformas de streaming y los modelos de lenguaje grandes como GPT y Gemini, que pueden generar texto coherente y responder preguntas complejas. La investigación continúa a un ritmo acelerado, explorando nuevas arquitecturas, abordando problemas de ética y seguridad, y empujando los límites de lo que las máquinas pueden aprender y hacer.\n",
    "\"\"\"\n",
    "\n",
    "PDF_FILENAME = \"historia_ia.pdf\"\n",
    "crear_pdf_de_ejemplo(PDF_FILENAME, texto_ia)\n",
    "\n",
    "# Extraer el texto del PDF que acabamos de crear\n",
    "texto_extraido = extraer_texto_de_pdf(PDF_FILENAME)\n",
    "\n",
    "print(\"\\n--- Inicio del Texto Extraído del PDF ---\")\n",
    "print(texto_extraido[:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2: Chunking (Fragmentación) del Texto**\n",
    "\n",
    "Ahora que tenemos el texto completo, debemos dividirlo en chunks. Una estrategia simple y efectiva es el **chunking de tamaño fijo**. Dividiremos el texto en fragmentos de un número determinado de caracteres.\n",
    "\n",
    "Elegir el tamaño del chunk es importante:\n",
    "- **Chunks demasiado grandes:** Pueden contener información irrelevante que diluya el contexto útil.\n",
    "- **Chunks demasiado pequeños:** Pueden carecer del contexto necesario para que el embedding capture el significado completo.\n",
    "\n",
    "Para este ejemplo, usaremos un tamaño de **2000 caracteres**, como se solicitó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_en_chunks(texto, chunk_size=2000):\n",
    "    \"\"\"Divide un texto en chunks de un tamaño específico.\"\"\"\n",
    "    return [texto[i:i + chunk_size] for i in range(0, len(texto), chunk_size)]\n",
    "\n",
    "CHUNK_SIZE = 2000\n",
    "text_chunks = dividir_en_chunks(texto_extraido, chunk_size=CHUNK_SIZE)\n",
    "\n",
    "print(f\"El texto ha sido dividido en {len(text_chunks)} chunks.\")\n",
    "print(\"\\n--- Ejemplo del Primer Chunk ---\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3: Generación de Embeddings**\n",
    "\n",
    "Este paso no cambia. Las funciones que creamos para generar embeddings pueden tomar cualquier cadena de texto, así que funcionarán perfectamente con nuestros nuevos chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para mapear nombres de modelos a sus dimensiones\n",
    "EMBEDDING_MODELS = {\n",
    "    \"openai\": {\"model\": \"text-embedding-3-small\", \"dimensions\": 1536},\n",
    "    \"gemini\": {\"model\": \"models/text-embedding-004\", \"dimensions\": 768},\n",
    "    \"cohere\": {\"model\": \"embed-multilingual-v3.0\", \"dimensions\": 1024}\n",
    "}\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = openai.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def get_gemini_embedding(text, model=\"models/text-embedding-004\"):\n",
    "    return genai.embed_content(model=model, content=text)[\"embedding\"]\n",
    "\n",
    "def get_cohere_embedding(text, model=\"embed-multilingual-v3.0\"):\n",
    "    response = co.embed(texts=[text], model=model)\n",
    "    return response.embeddings[0]\n",
    "\n",
    "print(\"Funciones de embedding definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 4: Almacenamiento en SQLite**\n",
    "\n",
    "Ahora, poblaremos nuestra base de datos SQLite con los chunks de texto del PDF y sus correspondientes embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"rag_database_pdf.db\"\n",
    "\n",
    "def setup_database():\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS text_embeddings\") # Empezar de cero\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE text_embeddings (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            model_name TEXT NOT NULL,\n",
    "            chunk TEXT NOT NULL,\n",
    "            embedding BLOB NOT NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"Base de datos '{DB_FILE}' y tabla 'text_embeddings' creadas.\")\n",
    "\n",
    "def store_embeddings(model_name, chunks, embeddings):\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        embedding_blob = np.array(embedding, dtype=np.float32).tobytes()\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO text_embeddings (model_name, chunk, embedding) VALUES (?, ?, ?)\",\n",
    "        (model_name, chunk, embedding_blob)\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# --- Proceso Principal de Población de la Base de Datos ---\n",
    "setup_database()\n",
    "\n",
    "chosen_model_name = \"gemini\" # Puedes cambiar a \"openai\" o \"cohere\"\n",
    "print(f\"\\nGenerando y almacenando embeddings para {len(text_chunks)} chunks usando el modelo: {chosen_model_name}...\")\n",
    "\n",
    "embedding_function = get_gemini_embedding # Asignamos la función directamente\n",
    "\n",
    "# Generamos los embeddings para cada chunk de texto\n",
    "embeddings_to_store = [embedding_function(chunk) for chunk in text_chunks]\n",
    "\n",
    "# Los almacenamos en la base de datos\n",
    "store_embeddings(chosen_model_name, text_chunks, embeddings_to_store)\n",
    "\n",
    "print(f\"¡Éxito! Se han almacenado {len(text_chunks)} chunks y sus embeddings en '{DB_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 5: Retrieval (Recuperación de Información)**\n",
    "\n",
    "La lógica de recuperación es idéntica a la del ejemplo anterior. La función buscará en la nueva tabla y comparará la query del usuario con los embeddings de los chunks del PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_top_k(query, model_name, top_k=2):\n",
    "    print(f\"Buscando los {top_k} chunks más relevantes para la query usando el modelo '{model_name}'...\")\n",
    "\n",
    "    embedding_function = get_gemini_embedding\n",
    "    query_embedding = embedding_function(query)\n",
    "    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n",
    "\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT chunk, embedding FROM text_embeddings WHERE model_name = ?\", (model_name,))\n",
    "    db_results = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not db_results:\n",
    "        print(\"No se encontraron embeddings en la base de datos.\")\n",
    "        return []\n",
    "\n",
    "    db_chunks = [row[0] for row in db_results]\n",
    "    embedding_dim = EMBEDDING_MODELS[model_name][\"dimensions\"]\n",
    "    db_embeddings = [np.frombuffer(row[1], dtype=np.float32) for row in db_results]\n",
    "    db_embeddings_np = np.array(db_embeddings)\n",
    "\n",
    "    similarities = cosine_similarity(query_embedding_np, db_embeddings_np)[0]\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    relevant_chunks = [db_chunks[i] for i in top_k_indices]\n",
    "    \n",
    "    print(\"Chunks recuperados con éxito.\")\n",
    "    return relevant_chunks\n",
    "\n",
    "# --- Prueba de Recuperación ---\n",
    "user_query = \"¿Quién fue Alan Turing y cuál fue su contribución a la IA?\"\n",
    "retrieved_chunks = retrieve_top_k(user_query, chosen_model_name, top_k=1)\n",
    "\n",
    "print(\"\\n--- Contexto Recuperado para la pregunta sobre Alan Turing ---\")\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 6: Augmentation y Generation**\n",
    "\n",
    "Finalmente, usamos los chunks recuperados del PDF para aumentar el prompt y generar una respuesta precisa y contextualizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, context_chunks):\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "    prompt = f\"\"\"\n",
    "Eres un asistente experto que responde preguntas sobre un documento. Responde a la pregunta del usuario basándote únicamente en el siguiente contexto extraído del documento.\n",
    "Si el contexto no contiene la información necesaria para responder, di explícitamente: 'La información no se encuentra en el documento proporcionado'.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta del usuario:\n",
    "{query}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_llm_response(prompt, provider=\"gemini\"):\n",
    "    print(f\"\\nEnviando prompt aumentado a {provider.upper()}...\")\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        elif provider == \"gemini\":\n",
    "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text\n",
    "        elif provider == \"cohere\":\n",
    "            response = co.chat(model='command-r', message=prompt)\n",
    "            return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error al contactar la API de {provider}: {e}\"\n",
    "\n",
    "# --- Proceso Final: Poniéndolo Todo Junto ---\n",
    "final_user_query = \"¿Qué fue la conferencia de Dartmouth y por qué es importante para la IA?\"\n",
    "context_chunks = retrieve_top_k(final_user_query, chosen_model_name, top_k=2)\n",
    "final_prompt = build_prompt(final_user_query, context_chunks)\n",
    "\n",
    "print(\"\\n--- Prompt Final Enviado al LLM ---\")\n",
    "print(final_prompt)\n",
    "\n",
    "# --- Generación con Gemini ---\n",
    "response_gemini = get_llm_response(final_prompt, provider=\"gemini\")\n",
    "print(\"\\n--- Respuesta de Gemini ---\")\n",
    "print(response_gemini)\n",
    "\n",
    "# --- Generación con OpenAI ---\n",
    "response_openai = get_llm_response(final_prompt, provider=\"openai\")\n",
    "print(\"\\n--- Respuesta de OpenAI ---\")\n",
    "print(response_openai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
