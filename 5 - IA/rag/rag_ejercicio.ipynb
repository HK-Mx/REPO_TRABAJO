{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0523580e",
   "metadata": {},
   "source": [
    "# üìù Ejercicio Pr√°ctico de RAG: An√°lisis Financiero de Gigantes Tecnol√≥gicos\n",
    "\n",
    "**¬°Bienvenido/a a tu primer gran proyecto como analista de IA!**\n",
    "\n",
    "## Misi√≥n\n",
    "\n",
    "Tu tarea es construir un sistema de Consulta y Respuesta (Q&A) utilizando RAG para analizar y comparar los √∫ltimos informes financieros anuales (conocidos como **10-K**) de cuatro de las empresas m√°s grandes del mundo: **Apple, Google (Alphabet), Microsoft y Meta**. \n",
    "\n",
    "Utilizar√°s las habilidades que has aprendido en los notebooks anteriores para cargar estos documentos desde la web, dividirlos en fragmentos (chunks), indexarlos en una base de datos vectorial con FAISS y construir una cadena de LangChain para hacerles preguntas complejas.\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "\n",
    "1.  Aplicar el flujo de trabajo de RAG a un caso de uso del mundo real con documentos complejos.\n",
    "2.  Manejar m√∫ltiples fuentes de datos y distinguirlas usando metadatos.\n",
    "3.  Practicar la construcci√≥n de una cadena RAG de alto nivel con LangChain.\n",
    "4.  Formular preguntas que requieran b√∫squeda, extracci√≥n y s√≠ntesis de informaci√≥n de distintos documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e49741",
   "metadata": {},
   "source": [
    "### **Paso 0: Instalaci√≥n y Configuraci√≥n**\n",
    "\n",
    "Aseg√∫rate de tener todas las librer√≠as necesarias y de configurar tus claves de API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta y ejecuta si es necesario\n",
    "# !pip install langchain langchain-openai langchain-google-genai langchain-community faiss-cpu beautifulsoup4\n",
    "\n",
    "import os\n",
    "\n",
    "# TODO: Configura tus claves de API aqu√≠\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"YOUR_GEMINI_API_KEY\"\n",
    "# os.environ[\"COHERE_API_KEY\"] = \"YOUR_COHERE_API_KEY\"\n",
    "\n",
    "print(\"Configuraci√≥n lista.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4d2ef",
   "metadata": {},
   "source": [
    "### **Parte 1: Recolecci√≥n de Datos (C√≥digo Proporcionado)**\n",
    "\n",
    "Te proporcionamos los enlaces a los √∫ltimos informes 10-K de cada empresa, directamente desde la base de datos EDGAR de la SEC (Comisi√≥n de Bolsa y Valores de EE. UU.).\n",
    "\n",
    "Usaremos `WebBaseLoader` de LangChain para cargar el contenido de estas URLs. Un paso **crucial** que hemos a√±adido es un bucle para a√±adir el nombre de la empresa a los **metadatos** de cada documento cargado. Esto ser√° fundamental m√°s adelante para saber de qu√© empresa proviene cada fragmento de informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = {\n",
    "    \"apple\": \"https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/aapl-20230930.htm\",\n",
    "    \"google\": \"https://www.sec.gov/Archives/edgar/data/1652044/000165204424000022/goog-20231231.htm\",\n",
    "    \"microsoft\": \"https://www.sec.gov/Archives/edgar/data/789019/000095017023035122/msft-20230630.htm\",\n",
    "    \"meta\": \"https://www.sec.gov/Archives/edgar/data/1326801/000132680124000012/meta-20231231.htm\"\n",
    "}\n",
    "\n",
    "all_documents = []\n",
    "print(\"Cargando documentos desde la web... Esto puede tardar un momento.\")\n",
    "\n",
    "for company, url in urls.items():\n",
    "    print(f\"Cargando informe de {company.capitalize()}...\")\n",
    "    loader = WebBaseLoader(url)\n",
    "    # Cargamos los documentos de la URL\n",
    "    company_docs = loader.load()\n",
    "    # A√±adimos el nombre de la empresa a los metadatos de cada documento\n",
    "    for doc in company_docs:\n",
    "        doc.metadata['company'] = company\n",
    "    all_documents.extend(company_docs)\n",
    "\n",
    "print(f\"\\nCarga completa. Total de documentos cargados: {len(all_documents)}\")\n",
    "print(\"Ejemplo de metadatos del primer documento:\", all_documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd5359",
   "metadata": {},
   "source": [
    "### **Parte 2: Chunking de los Documentos (¬°Tu Turno!)**\n",
    "\n",
    "Los informes 10-K son extremadamente largos. Necesitas dividirlos en chunks manejables. Usa `RecursiveCharacterTextSplitter` para esta tarea. Elige un tama√±o de chunk y un solapamiento que consideres adecuados para textos densos como estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# TODO: Instancia el RecursiveCharacterTextSplitter.\n",
    "# Pista: Un chunk_size entre 1500 y 2500 y un chunk_overlap de 100-200 suele funcionar bien.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# TODO: Divide los `all_documents` en `all_chunks`.\n",
    "all_chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Documentos divididos en {len(all_chunks)} chunks.\")\n",
    "print(\"\\n--- Ejemplo de un Chunk ---\")\n",
    "print(\"Contenido:\", all_chunks[100].page_content[:300] + \"...\")\n",
    "print(\"Metadatos:\", all_chunks[100].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310823e8",
   "metadata": {},
   "source": [
    "### **Parte 3: Creaci√≥n de la Base de Datos Vectorial (¬°Tu Turno!)**\n",
    "\n",
    "Ahora, convierte tus chunks en vectores y almac√©nalos en una base de datos FAISS. \n",
    "\n",
    "1.  Elige e instancia un modelo de embeddings (`OpenAIEmbeddings`, `GoogleGenerativeAIEmbeddings`, etc.).\n",
    "2.  Usa `FAISS.from_documents()` para crear el √≠ndice vectorial. ¬°Prep√°rate, este paso consumir√° llamadas a la API y puede tardar unos minutos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10550830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# O puedes usar: from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# TODO: 1. Instancia tu modelo de embeddings preferido.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"Generando embeddings y creando el vector store FAISS... Paciencia.\")\n",
    "# TODO: 2. Crea el vector store FAISS a partir de `all_chunks` y el modelo de `embeddings`.\n",
    "vectorstore = FAISS.from_documents(all_chunks, embeddings)\n",
    "\n",
    "print(\"\\n¬°Vector store creado con √©xito!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cae642",
   "metadata": {},
   "source": [
    "### **Parte 4: Construcci√≥n de la Cadena de RAG (¬°Tu Turno!)**\n",
    "\n",
    "Es hora de ensamblar la cadena que responder√° a nuestras preguntas. \n",
    "\n",
    "1.  Crea un `retriever` a partir del `vectorstore`.\n",
    "2.  Elige un `ChatModel` para la generaci√≥n de la respuesta.\n",
    "3.  Define un `prompt` (puedes usar `hub.pull(\"rlm/rag-prompt\")` o crear el tuyo).\n",
    "4.  Une todo usando el LangChain Expression Language (LCEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "# TODO: 1. Crea un retriever a partir del vectorstore. \n",
    "# Pista: .as_retriever() puede tomar argumentos como search_kwargs={\"k\": 3} para definir cu√°ntos chunks recuperar.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# TODO: 2. Instancia tu LLM preferido.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 3. Un prompt gen√©rico para RAG.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"Fuente: {doc.metadata.get('company', 'desconocida')}\\n{doc.page_content}\" for doc in docs)\n",
    "\n",
    "# TODO: 4. Define la cadena RAG usando LCEL. \n",
    "# La estructura `{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}` es un excelente punto de partida.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Cadena RAG lista para recibir preguntas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603866f",
   "metadata": {},
   "source": [
    "### **Parte 5: ¬°A Preguntar! El Momento de la Verdad**\n",
    "\n",
    "Usa la `rag_chain` que has construido para responder a las siguientes preguntas. Analiza las respuestas y piensa si son correctas y completas.\n",
    "\n",
    "**Consejo:** Si una respuesta no es buena, ¬°intenta reformular la pregunta! La forma en que preguntas (Prompt Engineering) es tan importante como el sistema RAG en s√≠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta_1 = \"Resume los principales factores de riesgo de negocio mencionados por Meta en su informe.\"\n",
    "print(f'--- Pregunta: {pregunta_1} ---')\n",
    "# TODO: Invoca la cadena con la pregunta 1\n",
    "# respuesta_1 = rag_chain.invoke(pregunta_1)\n",
    "# print(respuesta_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta_2 = \"¬øCu√°l fue el total de ingresos (Total Revenues) de Apple y Microsoft en su √∫ltimo a√±o fiscal reportado? Cita las cifras y comp√°ralas.\"\n",
    "print(f'--- Pregunta: {pregunta_2} ---')\n",
    "# TODO: Invoca la cadena con la pregunta 2\n",
    "# respuesta_2 = rag_chain.invoke(pregunta_2)\n",
    "# print(respuesta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03082550",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta_3 = \"Busca la frase 'Artificial Intelligence' en los documentos de Google y Microsoft. ¬øQu√© puedes inferir sobre la importancia que le dan a la IA bas√°ndote en el contexto donde aparece?\"\n",
    "print(f'--- Pregunta: {pregunta_3} ---')\n",
    "# TODO: Invoca la cadena con la pregunta 3\n",
    "# respuesta_3 = rag_chain.invoke(pregunta_3)\n",
    "# print(respuesta_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4baa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta_desafio = \"Bas√°ndote en las secciones de 'Risk Factors', ¬øqu√© riesgo com√∫n relacionado con la ciberseguridad o la privacidad de datos mencionan tanto Apple como Meta?\"\n",
    "print(f'--- Pregunta de Desaf√≠o: {pregunta_desafio} ---')\n",
    "# TODO: Invoca la cadena con la pregunta de desaf√≠o\n",
    "# respuesta_desafio = rag_chain.invoke(pregunta_desafio)\n",
    "# print(respuesta_desafio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6432b259",
   "metadata": {},
   "source": [
    "## Conclusi√≥n y Pistas Finales\n",
    "\n",
    "¬°Enhorabuena! Has completado un proyecto de RAG de principio a fin sobre un caso de uso complejo y realista. \n",
    "\n",
    "**Para reflexionar:**\n",
    "* ¬øFueron las respuestas siempre correctas? ¬øEn qu√© casos fall√≥ el sistema?\n",
    "* ¬øC√≥mo podr√≠a mejorarse el sistema? (¬øChunks m√°s grandes/peque√±os? ¬øOtro modelo de embedding? ¬øUn prompt m√°s detallado?).\n",
    "* La funci√≥n `format_docs` que te dimos incluye el nombre de la empresa. ¬øC√≥mo te ayud√≥ esto a verificar las respuestas del LLM?\n",
    "\n",
    "Este ejercicio demuestra que la potencia de los LLMs se multiplica cuando los conectamos a datos espec√≠ficos y relevantes. ¬°Ahora tienes la base para construir aplicaciones de IA mucho m√°s potentes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
