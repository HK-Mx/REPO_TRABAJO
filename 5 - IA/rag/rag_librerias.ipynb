{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📔 Jupyter Notebook: RAG de Alto Nivel con LangChain y FAISS\n",
    "\n",
    "## Objetivo de la Clase\n",
    "\n",
    "Tras haber construido un sistema RAG desde cero, ahora vamos a resolver el mismo problema utilizando herramientas estándar de la industria: **LangChain** y **FAISS**.\n",
    "\n",
    "El objetivo es comprender cómo los frameworks de alto nivel abstraen la complejidad y nos permiten construir sistemas RAG robustos y eficientes con mucho menos código. Compararemos directamente los componentes de LangChain con las funciones que creamos manualmente.\n",
    "\n",
    "### De lo Manual a LangChain: Un Mapeo\n",
    "\n",
    "| Tarea | Nuestra Implementación Manual | Abstracción en LangChain |\n",
    "| :--- | :--- | :--- |\n",
    "| **Cargar Datos (PDF)** | `extraer_texto_de_pdf()` | `Document Loaders` (ej. `PyMuPDFLoader`) |\n",
    "| **Fragmentar Texto** | `dividir_en_chunks()` | `Text Splitters` (ej. `RecursiveCharacterTextSplitter`) |\n",
    "| **Generar Embeddings** | `get_gemini_embedding()`, etc. | `Embeddings Wrappers` (ej. `GoogleGenerativeAIEmbeddings`) |\n",
    "| **Almacenar y Buscar** | `SQLite` + `cosine_similarity` | `Vector Stores` (ej. `FAISS`) |\n",
    "| **Formatear Prompt** | `build_prompt()` | `Prompt Templates` (ej. `ChatPromptTemplate`) |\n",
    "| **Llamar al LLM** | `get_llm_response()` | `LLM/ChatModel Wrappers` (ej. `ChatGoogleGenerativeAI`) |\n",
    "| **Orquestar el Flujo**| Script lineal | `Chains` (LCEL - LangChain Expression Language) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 0: Instalación de Dependencias**\n",
    "\n",
    "Las dependencias de LangChain son modulares. Necesitamos instalar el paquete principal y los específicos para los componentes que usaremos (OpenAI, Gemini, Cohere, FAISS, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta y ejecuta la siguiente línea si no tienes instaladas las librerías\n",
    "# !pip install langchain langchain-openai langchain-google-genai langchain-cohere langchain-community faiss-cpu pypdf fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 1: Configuración de APIs y Creación del PDF**\n",
    "\n",
    "Este paso inicial es similar. Configuramos nuestras claves de API y creamos el mismo documento PDF de ejemplo para tener una fuente de datos consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# --- CONFIGURACIÓN DE LAS API KEYS ---\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"YOUR_GEMINI_API_KEY\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"YOUR_COHERE_API_KEY\"\n",
    "\n",
    "print(\"API keys configuradas.\")\n",
    "\n",
    "# --- CREACIÓN DEL PDF DE EJEMPLO ---\n",
    "# Reutilizamos la función y el texto del notebook anterior\n",
    "def crear_pdf_de_ejemplo(nombre_archivo, texto):\n",
    "    doc = fitz.open()\n",
    "    pagina = doc.new_page()\n",
    "    rect = fitz.Rect(50, 50, 550, 800)\n",
    "    # PyMuPDF > 1.24.2 usa insert_textbox en lugar de insert_text\n",
    "    try:\n",
    "        pagina.insert_textbox(texto, rect, fontsize=12, fontname=\"helv\")\n",
    "    except AttributeError:\n",
    "        pagina.insert_text(texto, rect, fontsize=12, fontname=\"helv\")\n",
    "    doc.save(nombre_archivo)\n",
    "    doc.close()\n",
    "\n",
    "texto_ia = \"\"\"\n",
    "Historia de la Inteligencia Artificial\n",
    "\n",
    "La historia de la inteligencia artificial (IA) es fascinante y se remonta a la antigüedad... [El mismo texto largo del notebook anterior]\n",
    "\"\"\"\n",
    "PDF_FILENAME = \"historia_ia_langchain.pdf\"\n",
    "crear_pdf_de_ejemplo(PDF_FILENAME, texto_ia)\n",
    "print(f\"PDF '{PDF_FILENAME}' creado para el ejemplo con LangChain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 2: Carga y División de Datos con LangChain**\n",
    "\n",
    "Aquí vemos la primera gran simplificación. Usamos un `DocumentLoader` para cargar el PDF y un `TextSplitter` para dividirlo en chunks. LangChain maneja la iteración sobre las páginas y la lógica de la división."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Cargar el documento\n",
    "loader = PyMuPDFLoader(PDF_FILENAME)\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Dividir el documento en chunks\n",
    "# RecursiveCharacterTextSplitter es más robusto que un split simple.\n",
    "# Intenta dividir por párrafos, luego por líneas, etc.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100  # Un pequeño solapamiento ayuda a mantener el contexto entre chunks\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Documento cargado y dividido en {len(chunks)} chunks.\")\n",
    "print(\"\\n--- Metadatos del Primer Chunk ---\")\n",
    "print(chunks[0].metadata)\n",
    "print(\"\\n--- Contenido del Primer Chunk ---\")\n",
    "print(chunks[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 3: Embeddings y Creación del Vector Store (FAISS)**\n",
    "\n",
    "Este es el paso más impactante. Con una sola línea de código, generamos los embeddings para todos los chunks y los indexamos en un **vector store FAISS** en memoria.\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** es una librería altamente optimizada para la búsqueda eficiente de similitud entre vectores. LangChain se integra perfectamente con ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Seleccionamos el modelo de embeddings (podríamos usar OpenAIEmbeddings o CohereEmbeddings también)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "# Creamos el Vector Store FAISS a partir de los chunks y el modelo de embeddings\n",
    "# ¡Esta línea reemplaza toda nuestra lógica de base de datos y serialización!\n",
    "print(\"Creando el vector store FAISS...\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Vector store creado con éxito.\")\n",
    "\n",
    "# FAISS puede guardarse en disco y cargarse después\n",
    "# vectorstore.save_local(\"faiss_index\")\n",
    "# new_vectorstore = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 4: Creación de la Cadena RAG con LCEL**\n",
    "\n",
    "**LCEL (LangChain Expression Language)** es la forma moderna y declarativa de construir cadenas en LangChain. Nos permite \"unir\" componentes con el símbolo `|` (pipe).\n",
    "\n",
    "Construiremos una cadena que:\n",
    "1.  Toma una pregunta.\n",
    "2.  Usa un **retriever** (creado desde nuestro vector store) para obtener los documentos relevantes.\n",
    "3.  Formatea un prompt con la pregunta y los documentos recuperados.\n",
    "4.  Pasa el prompt al LLM.\n",
    "5.  Parsea la salida para obtener una respuesta limpia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "# 1. Definimos el LLM que generará la respuesta\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# 2. Creamos un \"retriever\" desde nuestro vector store\n",
    "# Esto reemplaza nuestra función `retrieve_top_k`\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# 3. Usamos un prompt de la comunidad de LangChain (o creamos uno propio)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 4. Creamos la cadena usando LCEL\n",
    "def format_docs(docs):\n",
    "    # Formatea los documentos recuperados en un solo string\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"Cadena RAG creada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 5: Ejecución de la Cadena y Comparación de Modelos**\n",
    "\n",
    "Ahora, simplemente \"invocamos\" nuestra cadena con una pregunta. LangChain se encarga de todo el proceso de RAG de forma transparente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"¿Quiénes son considerados los 'padrinos de la IA' y por qué?\"\n",
    "\n",
    "print(f\"Ejecutando la cadena con la query: '{user_query}'\")\n",
    "print(\"--- RESPUESTA (Google Gemini) ---\")\n",
    "\n",
    "# Invocamos la cadena y obtenemos la respuesta\n",
    "response = rag_chain.invoke(user_query)\n",
    "print(response)\n",
    "\n",
    "# --- Probando con OpenAI (ejemplo de modularidad) ---\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "print(\"\\n--- Reconfigurando la cadena para usar OpenAI ---\")\n",
    "\n",
    "# 1. Cambiamos el modelo de embeddings y creamos un nuevo vector store\n",
    "embeddings_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore_openai = FAISS.from_documents(chunks, embeddings_openai)\n",
    "\n",
    "# 2. Cambiamos el LLM\n",
    "llm_openai = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 3. Creamos el nuevo retriever y la nueva cadena\n",
    "retriever_openai = vectorstore_openai.as_retriever()\n",
    "rag_chain_openai = (\n",
    "    {\"context\": retriever_openai | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_openai\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- RESPUESTA (OpenAI GPT-4o-mini) ---\")\n",
    "response_openai = rag_chain_openai.invoke(user_query)\n",
    "print(response_openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones Finales: ¿Por qué usar un Framework?\n",
    "\n",
    "Como hemos visto, LangChain y FAISS reducen drásticamente la cantidad de código y la complejidad para construir un sistema RAG.\n",
    "\n",
    "- **Velocidad de Desarrollo:** Pasamos de implementar manualmente la lógica de la base de datos, el chunking y la recuperación a usar componentes pre-construidos y optimizados.\n",
    "- **Robustez y Mantenimiento:** El código es más limpio, más declarativo y más fácil de mantener. Aprovechamos las mejoras y correcciones de errores de la comunidad de LangChain.\n",
    "- **Rendimiento:** FAISS proporciona una búsqueda de similitud mucho más rápida que nuestro cálculo manual de similitud del coseno sobre todos los elementos, lo cual es crucial para grandes volúmenes de datos.\n",
    "- **Modularidad:** Cambiar un componente (como el LLM o el modelo de embeddings) es trivial, como demostramos al pasar de Gemini a OpenAI.\n",
    "\n",
    "Sin embargo, la lección más importante es que **haber construido el sistema manualmente primero nos da una comprensión profunda de lo que estas herramientas hacen por debajo**. Este conocimiento es invaluable para depurar problemas, optimizar el rendimiento y tomar decisiones informadas sobre qué componentes usar en un proyecto real."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
